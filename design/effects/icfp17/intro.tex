
\section{Introduction}


Good software is distinguished from bad software by qualities such as security, maintainability, and performance. We are interested in how the design of a programming language and its type system can make it easier to write secure software.

There are different situations where we may not trust code. One example is in a development environment adhering to ideas of \textit{code ownership}, wherein developers may exercise responsibility for specific components of the system \cite{bird11}. When a developer writes code to interact with another component, they can make false assumptions about how it should be used. This can break correctness or leave components in a malconfigured state, putting the whole system at risk. Another setting involves applications which allow third-party plug-ins, some of which could be malicious. Such an example is a web mash-up, which brings together several disparate web applications into one unified service. In both cases, despite the presence of untrustworthy components, we want the system to function securely.

It is difficult to determine if a piece of code should be trusted, but a range of approaches can be taken about the issue. One is to \textit{sandbox} untrusted code inside a virtual environment. If anything goes wrong, damage is theoretically limited to the virtual environment, but in practice there are many vulnerabilities \cite{coker15, maass16, watson07, schreuders13}. Verification gives a comprehensive analysis of the behaviour of code, but the techniques are heavyweight and developers must have a deep understanding of how they work in order to use them \cite{kneuper97}. Furthermore, verification requires a complete specification of the system, which may be undefined or evolving throughout the development process. Lightweight analyses, such as type systems, are easy for the developer to use, but existing languages are insufficient in the tools they provide for detecting and isolating untrustworthy components \cite{chen07, ter-louw08}. A qualitative approach might be taken, where software is developed according to best-practice guidelines. One such guideline is the \textit{principle of least authority}: that software components should only have access to the information and resources necessary for their purpose \cite{saltzer74}. For example, a logger module, which only needs to append to a file, should not be given arbitrary read-write access. Another is \textit{privilege separation}, where the division of a program into components is informed by what resources are needed and how they are to be propagated \cite{saltzer75}. This report focuses on the class of lightweight analyses, and in particular how type systems can be used to reject unsafe programs and put developers in a more informed position to make qualitative assessments.

One approach to privilege separation is the capability model. A \textit{capability} is an unforgeable token granting its bearer permission to perform some operation \cite{dennis66}. For example, a system resource like a file or socket can only be used through a capability granting operations on it. Capabilities also encapsulate the source of \textit{effects}, which describe intensional details about the way in which a program executes \cite{nielson99}. For example, a logger might $\kwa{append}$ to a $\kwa{File}$, and so executing its code would incur a $\kwa{File.append}$ effect. In the capability model, this requires the logger to possess a capability granting it the ability to append to that particular file.

Although the idea of a capability is an old one, there has been recent interest in its application to programming language design. Miller has identified the ways in which capabilities should proliferate to encourage \textit{robust composition} --- a set of ideas summarised by ``only connectivity begets connectivity'' \cite{miller06}. As a result, a program's components are explicitly parameterised by the capabilities they may use; the only effects a component can incur are those for which it has been given a capability. Building on these ideas, Maffeis et. al. formalised the notion of a \textit{capability-safe} language and showed a subset of Caja (a JavaScript implementation) is capability-safe \cite{maffeis10}. Another capability-safe language, and one we use in this report, is Wyvern \cite{nistor13}.

Effect systems were introduced by Lucassen and Gifford to determine what code can be safely parallelised \cite{lucassen88}. They have also been applied to problems such as determining which functions might be invoked in a program \cite{tang94} or determining which regions in memory may be accessed or updated \cite{talpin94}. Knowing what effects a piece of code might incur allows a developer to determine if code is trustworthy before executing it. This can be qualitatively assessed by comparing the static approximation of its effects to its expected least authority --- a ``logger'' implementation which could write to a $\kwa{Socket}$ is not to be trusted!

Despite these benefits, effect systems have seen little use in mainstream programming languages. Rytz et. al. believe the verbosity of their annotations is the main reason \citep{rytz12}. Successive works have focussed on reducing the developer overhead through techniques such as inference. Inference enables developers to rapidly prototype without annotations, incrementally adding them as their safety needed, but the benefit of capabilities for this has received less attention: because capabilities encapsulate the source of effects, and because capability-safety impose constraints on how they can propagate through the system, the effects of a piece of code can be safely approximated by inspecting what capabilities are passed into it. This is the key contribution of this report: capability-safety facilitates a lightweight, incremental effect discipline.

We begin by discussing preliminary concepts involving the formal definition of programming languages, effect systems, and Miller's capability model. Chapter 3 introduces the Operation Calculus $\opercalc$, a typed lambda calculus with a simple notion of capabilities and their operations in which all code is effect-annotated. Relaxing this requirement, we then introduce the Capability Calculus $\epscalc$, which permits the nesting of unannotated code inside annotated code in a controlled, capability-safe manner. A safe inference about the unannotated code can be made by inspecting the capabilities passed into it from its annotated surroundings. In chapter 4 we show how $\epscalc$ can model practical examples. We finish with a summary and a literature comparison.



