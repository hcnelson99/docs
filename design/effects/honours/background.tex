\chapter{Background}\label{C:background}

In this chapter we cover the necessary concepts and existing work informing this report. First we detail how a programming language and its type system are defined, and how to prove the type system is correct. For this purpose, we present a toy language called $\calc$. We then summarise a variant of the simply-typed lambda calculus $\stlc$. $\stlc$ is an historically important model of computation which serves as a basis for many programming languages, including the capability calculus $\epscalc$. $\epscalc$ is also a capability-based language with an effect system. To understand what this means we cover some existing work on effect systems and discuss Miller's capability model.

\section{Formally Defining a Programming Language}

A programming language can be defined by giving three sets of rules: a grammar, which defines syntactically legal terms; dynamic rules, which give the meaning of a program by how it is executed; and static rules, which determine whether programs meet certain well-behavedness properties. When a language has been defined we want to know its static rules are mathematically correct with respect to the dynamic rules.

Alongside the explanation of these concepts we develop Expression-based Language ($\calc$), a simple, typed language of arithmetic and boolean expressions. It is a language invented in this report for demonstrative purposes. Like every language we cover, it is expression-based, meaning that programs are evaluated to yield a value. Although $\calc$ is not very interesting, it will illustrate our general approach.

\subsection{Grammar}

The grammar of a language specifies what strings are syntactically legal. A syntactically legal string is called a \textit{term}. It is specified by giving the different categories of terms and the forms which instantiate those categories. The conventions for specifying a grammar are based on standard Backur-Naur form \cite{bnf}. Figure \ref{fig:ebl_grammar} shows a simple grammar describing integer literals and arithmetic expressions on them. In each rule, the metavariables range over the terms of the category for which they are named.

A $\calc$ program is an expression $e$, consisting of variable definitions, constants, and the application of boolean and arithmetic operators. A valid expression is either a variable, a constant (such as $3$, $0$, $\true$, or $\false$), the application of an operator $+$ or $\lor$ to two subexpressions, or a binding for a variable in a piece of code ($\kwa{let}$ expression). The following are $\calc$ terms: $x$, $y$, $3$, $3+2$, $\false \lor \true$, $3 \lor \false$, $\true + \false$, $\letxpr{x}{3}{x+1}$.

Although the grammar hs no brackets, a string like $3 + (x + 2)$ should be seen as a short-hand for the corresponding abstract syntax tree (AST), whose structure is given by the rules of the grammar. For some strings the AST is ambiguous, as in $3 + x + 2$, which might be parsed as $3 + (x + 2)$ or as $(3 + x) + 2$. How we parse and disambiguate strings is not relevant to us, so throughout the report we only ever consider strings which unambiguously correspond to terms in the grammar.\\

\begin{figure}[h]

\[
\begin{array}{c}

\begin{array}{lllr}

e & ::= & ~ & exprs: \\
	& | & x & variable \\
	& | & e + e & addition \\
	& | & e \lor e & disjunction \\
	& | & \letxpr{x}{e}{e} & let~expr. \\
	&&\\
	
v & ::= & ~ & values: \\
	& | & l & \Nat~constant \\
	& | & b & \Bool~constant \\
	&&\\

\end{array}

\end{array}
\]

\vspace{-12pt}
\caption{Grammar for $\calc$ expressions.}
\label{fig:ebl_grammar}
\end{figure}


\subsection{Dynamic Rules}

The dynamic rules of a language specify the meaning of terms. There are different approaches, but the one we use is called \textit{small-step semantics}, where the meaning of a program is given by explaining how it is executed. This is given as a set of \textit{inference rules}, which are given as a set of premises above a dividing line. If the premises above the line hold, they imply the result below the line. The results are called \textit{judgements}. If an inference rule has no premises it is an \textit{axiom}. A particular application of an inference rule is a \textit{derivation}. Figure \ref{fig:ebl_dynamic} gives the dynamic rules for $\calc$, which specify a binary relation $\longrightarrow$, representing a single computational step. When the relation holds of a particular pair, we say the judgement $e \longrightarrow e'$ holds, and that $e$ reduces to $e'$. 

\begin{figure}[h]

\noindent
\fbox{$e \longrightarrow e$}

\[
\begin{array}{c}

\infer[\textsc{(E-Add1)}]
	{e_1 + e_2 \longrightarrow e_1' + e_2}
	{e_1 \longrightarrow e_1'}
~~
\infer[\textsc{(E-Add2)}]
	{l_1 + e_2 \longrightarrow l_1 + e_2'}
	{e_2 \longrightarrow e_2'}
~~
\infer[\textsc{(E-Add3)}]
	{l_1 + l_2 \longrightarrow l_3}
	{l_1 + l_2 = l_3} \\[4ex]

\infer[\textsc{(E-Or1)}]
	{e_1 \lor e_2 \longrightarrow e_1' \lor e_2}
	{e_1 \longrightarrow e_1'}
	~~~
\infer[\textsc{(E-Or2)}]
	{\true \lor e_2 \longrightarrow \true}
	{}
	~~~
\infer[\textsc{(E-Or3)}]
	{\false \lor e_2 \longrightarrow e_2}
	{}\\[4ex]
	
\infer[\textsc{(E-Let1)}]
	{\letxpr{x}{e_1}{e_2} \longrightarrow \letxpr{x}{e_1'}{e_2}}
	{e_1 \longrightarrow e_1'}
	~~~
\infer[\textsc{(E-Let2)}]
	{\letxpr{x}{v}{e_2} \longrightarrow [v/x]e_2}
	{}

\end{array}
\]

\vspace{-12pt}
\caption{Inference rules for single-step reductions.}
\label{fig:ebl_dynamic}
\end{figure}

An addition is reduced by first reducing the left-hand side to an irreducible form (\textsc{E-Add1}) and then the right-hand side (\textsc{E-Add2}). If both sides are integer literals, the expression reduces to whatever is the sum of those literals.

According to these rules, a disjunction is reduced by first reducing the left-hand side to an irreducible form (\textsc{E-Or1}). If the left-hand side is the boolean literal $\true$, the expression reduces to $\true$ (because $\true \lor Q = \true$). Otherwise if the left-hand side is the boolean literal $\false$, the expression reduces to the right-hand side $e_2$ (because $\false \lor Q = Q$). This particular formulation encodes short-circuiting behaviour into $\lor$, meaning if the left-hand side is true, the whole expression will evaluate to true without checking the right-hand side.

A $\kwa{let}$ expression is reduced by first reducing the subexpression being bound (\textsc{E-Let1}). If the subexpression is an irreducible form $v_1$, the variable $x$ is substituted for $v_1$ in the body $e_2$ of the $\kwa{let}$ expression. The notation for this is $[v_1/x]e_2$. For example, $\letxpr{x}{1}{x+1}$ reduces to $1+1$ by \textsc{E-Let2}.

Formally, substitution is a function operating on expressions. A definition is given in Figure \ref{fig:ebl_sub_defn}. The notation $[e_1/x]e$ is short-hand for $\kwa{substitution}(e, e_1, x)$. For multiple substitutions we use the notation $[e_1/x_1, e_2/x_2] e$ as shorthand for $[e_2/x_2]([e_1/x_1] e)$. Note how the order of the variables has been flipped; the substitutions occur left-to-right, as they are written.

\begin{figure}[h]

\bm{$\kwa{substitution :: e \times e \times v \rightarrow e}$}

\begin{itemize}
	\setlength\itemsep{-0.7em}
	\item[] $[e'/y]l = l$
	\item[] $[e'/y]b = b$ 
	\item[] $[e'/y]x =  v$, if $x = y$
	\item[] $[e'/y]x = x$, if $x \neq y$
	\item[] $[e'/y](e_1 + e_2) = [e'/y]e_1 + [e'/y]e_2$
	\item[] $[e'/y](e_1 \lor e_2) = [e'/y]e_1 \lor [e'/y]e_2$
	\item[] $[e'/y](\letxpr{x}{e_1}{e_2}) = \letxpr{x}{[e'/y]e_1}{[e'/y]e_2}$, if $y \neq x$ and $y$ does not occur free in $e_1$ or $e_2$
\end{itemize}

\vspace{-12pt}
\caption{Substitution for $\calc$.}
\label{fig:ebl_sub_defn}
\end{figure}

A robust definition of the $\kwa{substitution}$ function is surprisingly tricky. Consider the program $\letxpr{x}{1}{(\letxpr{x}{2}{x+z})}$. It contains two different variables with the same name $x$, with the inner one ``shadowing'' the outer one. Neither variable occurs ``free'', because both have been introduced in the body of the program (one for each $\kwa{let}$). Such variables are called bound variables. By contrast, $z$ is a free variable because it has no definition in the program. A robust $\kwa{substitution}$ should not accidentally conflate two different variables with identical names, and it should not do anything to bound variables.

To illustrate the solution, consider $\letxpr{x}{1}{(\letxpr{x}{2}{x+z})}$. In some sense, this is an equivalent program to $\letxpr{x}{1}{(\letxpr{y}{2}{y+z})}$. Because the names of variables are arbitrary, changing them will not change the semantics of the program. Therefore, we freely and implicitly interchange expressions which are equivalent up to the naming of bound variables. This process is called $\alpha$-conversion \cite[p. 71]{tapl}. Consequently, we assume variables are (re-)named in this way to avoid these problems and to play nicely with the definition of $\kwa{substitution}$.

Lastly, note how in an expression like $\letxpr{x}{1+1}{x+1}$. According to the rules, $1+1$ would first be reduced to $2$ before the substitution is made on $x+1$. This strategy of reducing expressions to irreducible forms before they are bound to their names is known as \textit{call-by-value}. Some languages --- such as Haskell --- are not call-by-value, but we shall only consider languages with call-by-value semantics.

From the single-step reduction relation, we define a multi-step reduction relation as a sequence of zero\footnote{We permit multi-step reductions of length zero to be consistent with Pierce, who defines multi-step reduction as a reflexive relation \cite[p. 39]{tapl}.} or more single-steps. This is written $e \longrightarrow^* e'$. For example, if $e_1 \longrightarrow e_2$ and $e_2 \longrightarrow e_3$, then $e_1 \longrightarrow^* e_3$. Figure \ref{fig:ebl_dyn_multistep} defines multi-step reduction in $\calc$.


\begin{figure}[h]

\noindent
\fbox{$e \longrightarrow^{*} e$}

\[
\begin{array}{c}

\infer[\textsc{(E-MultiStep1)}]
	{ e \longrightarrow^{*}  e}
	{}
~~~
\infer[\textsc{(E-MultiStep2)}]
	{ e \longrightarrow^{*}  e'}
	{ e \longrightarrow  e'} \\[3ex]
	
\infer[\textsc{(E-MultiStep3)}]
	{e \longrightarrow^{*}  e''}
	{ e \longrightarrow^{*}  e' &  e' \rightarrow^{*}  e''}
\end{array}
\]
\vspace{-12pt}
\caption{Dynamic rules.}
\label{fig:ebl_dyn_multistep}
\end{figure}





\subsection{Static Rules}

When attempting to reduce $\calc$ terms you may find you end up with nonsense, or get stuck in a situation where no rule applies due to a typing error. For example, $\false \lor 3 \longrightarrow 3$ by \textsc{E-Or3}, which is nonsense. $(1+1)+\false \longrightarrow 2 + \false$ by \textsc{E-Add1}, but then you are stuck because $+$ is an operation on numbers, and $\false$ is a boolean. Another example is $\kwa{x+1}$, which gets stuck because $x$ is undefined.

We often want to consider those programs which satisfy certain well-behavedness properties. One such property is that of being \textit{well-typed}: if a program is well-typed then during execution it will never get \textit{stuck} due to type-errors. Another says that every variable in a program must be declared before it is used. If a program satisfies these well-behavedness properties, its execution will never get stuck or produce a nonsense answer. We also want to know if a program satisfies these properties before it is executed.

To achieve this we add static rules, enriching $\calc$ with a basic type system, which associates each expression with a type. If an expression can be given a type then its execution will have no type errors. Our type system will also encode the requirement that variables be defined before they are used. The relevant constructrs for the type system are given as a grammar in Figure \ref{fig:calc_types}. There are two types: $\Nat$ and $\Bool$, and a notion of a typing context, which map variables to their types. This is needed in a program like $\letxpr{x}{1}{x+1}$, where in typing $x+1$, we need to know the type of $x$.

\begin{figure}[h]

\[
\begin{array}{c}

\begin{array}{lllr}

\tau & ::= & ~ & types: \\
	& | & \kwa{Nat} \\
	& | & \kwa{Bool} \\
	&&\\
	
\Gamma & ::= & ~ & contexts: \\
	& | & \varnothing \\
	& | & \Gamma, x: \tau \\
	&&\\
\end{array}

\end{array}
\]

\vspace{-12pt}
\caption{Grammar for the type system of $\calc$.}
\label{fig:calc_types}
\end{figure}

Figure \ref{fig:ebl_static} presents the static rules of $\calc$. The judgement form is $\Gamma \vdash e: \tau$, which means expression $e$ has type $\tau$ in the context $\Gamma$. When a judgement can be derived from the empty context it is written $\vdash e: \tau$ instead of $\varnothing \vdash e: \tau$.

\begin{figure}[h]

\noindent
\fbox{$\Gamma \vdash e: \tau$}

\[
\begin{array}{c}

\infer[\textsc{(T-Var)}]
	{\Gamma, x: \kwa{Int} \vdash x: \kwa{Int}}
	{}
~~~
\infer[\textsc{(T-Bool)}]
	{\vdash b : \Bool}
	{}
	~~~
\infer[\textsc{(T-Nat)}]
	{\vdash l : \Nat}
	{}\\[2ex]

	~~~
\infer[\textsc{(T-Or)}]
	{\Gamma \vdash e_1 \lor e_2 : \Bool}
	{\Gamma \vdash e_1: \Bool & \Gamma \vdash e_2: \Bool}
	~~~
\infer[\textsc{(T-Add)}]
	{\Gamma \vdash e_1 + e_2 : \Nat}
	{\Gamma \vdash e_1: \Nat & \Gamma \vdash e_2: \Nat} \\[2ex]
	
\infer[\textsc{(T-Let)}]
	{\Gamma \vdash \letxpr{x}{e_1}{e_2} : \tau_2}
	{\Gamma \vdash e_1: \tau_1 & \Gamma, x: \tau_1 \vdash e_2: \tau_2}
	
	
\end{array}
\]

\vspace{-12pt}
\caption{Inference rules for typing arithmetic expressions.}
\label{fig:ebl_static}
\end{figure}

\textsc{T-Bool} and \textsc{T-Nat} are rules which say that constants always type to $\Bool$ or $\Nat$. \textsc{T-Var} says that a variable types to whatever the context binds it to. \textsc{T-Or} types a disjunction if the arguments are both $\Bool$. \textsc{T-Add} types a sum if the arguments are both $\Nat$. The most interesting rule is \textsc{T-Let}, where the context gains a binding for $x$ to type-check the body of the $\kwa{let}$ expression. This lets $\letxpr{x}{1}{x+1}$ typecheck, because $x:\Int \vdash x + 1: \Int$. A derivation is given in Figure \ref{fig:ebl_let_tree}. The type of a $\kwa{let}$ expression is the type of its body.

\begin{figure}[h]


    \begin{prooftree*}
        \Infer0[\textsc{(T-Nat)}]{\vdash 1: \Nat}
        
        \Infer0[\textsc{(T-Var)}]{x: \Int \vdash x: \Int}
        \Infer0[\textsc{(T-Nat)}]{x: \Int \vdash 1: \Int }
        \Infer2[\textsc{(T-Add)}]{x: \Int \vdash x + 1: \Int}
        
        \Infer2[\textsc{(T-Let)}]{\vdash \letxpr{x}{1}{x+1}: \Int}
        
 	\end{prooftree*}
 	
\vspace{-12pt}
\caption{Derivation tree for $\letxpr{x}{1}{x+1}$}
\label{fig:ebl_let_tree}
\end{figure}
 
There are some pesky technicalities about typing contexts which need to be addressed. Though we have defined $\Gamma$ syntactically as a sequence of variable-type pairs, we really want to treat it as a mapping from variables to types. $x: \kwa{Int}, y: \kwa{Int}$ is really the same thing as $y: \kwa{Int}, x: \kwa{Int}$. Furthermore, if a judgement holds in a context $\Gamma$, it should also hold in any super-context $\Gamma$. For example, $x:\Int \vdash x:\Int$, but it's also true that $x:\Int, y:\Int \vdash x:\Int$. We can ensure these properties with the rules in Figure \ref{fig:ctx_rules}.

\begin{figure}[h]

\noindent
\fbox{$\Gamma \vdash e: \tau$}

\[
\begin{array}{c}

\infer[\textsc{($\Gamma$-Permute)}]
	{\Gamma' \vdash e: \tau}
	{\Gamma \vdash e: \tau & \Gamma'~is~a~permutation~of~\Gamma}
	~~~
\infer[\textsc{($\Gamma$-Widen)}]
	{\Gamma, x: \tau' \vdash e: \tau }
	{\Gamma \vdash e: \tau & x \notin \kwa{dom}(\Gamma)}

	
\end{array}
\]

\vspace{-12pt}
\caption{Structural rules for typing contexts.}
\label{fig:ctx_rules}
\end{figure}

\textsc{$\Gamma$-Permute} says that a judgement holds in $\Gamma$ if it holds in any permutation of $\Gamma$, meaning the order is irrelevant. \textsc{$\Gamma$-Widen} says that any judgement which holds in $\Gamma$ will hold in $\Gamma, x: \tau$, provided $x$ is not already in the domain of $\Gamma$. $\kwa{dom}(\Gamma)$ is the set of variables bound in $\Gamma$; a definition is given in \ref{fig:ctx_dom_defn}. Another property we desire of $\Gamma$ is that it contains no duplicate variables. However, by the convention of $\alpha$-renaming, all programs have unique variable names, so no rule is required.

\begin{figure}[h]

\bm{$\kwa{dom :: \Gamma \rightarrow \{ x \}}$}

\begin{itemize}
	\setlength\itemsep{-0.7em}
	\item[] $\kwa{dom}(\varnothing) = \varnothing$
	\item[] $\kwa{dom}(\Gamma, x: \tau) = \kwa{dom}(\Gamma) \cup \{ x \}$
\end{itemize}

\vspace{-12pt}
\caption{Definition of $\kwa{dom}$.}
\label{fig:ctx_dom_defn}
\end{figure}

These rules cause typing contexts to behave as we expect, but in practice the notation for contexts and how to manipulate are so conventional that we shall not bother to mention them again. We shall implicitly and automatically apply these rules.

It is worth mentioning that most languages have a \textit{subtyping} judgement, written $\tau_1 <: \tau_2$, meaning expressions of type $\tau_1$ may be provided anywhere in a program where an expression of type $\tau_2$ are expected, and the program will still be well-typed. $\calc$ has no subtyping rules, but we shall encounter some later.

\subsection{Soundness}

Having defined the static rules of $\calc$ we can try to apply the rules to those examples in the last section which got stuck during reduction or evaluated to some nonsense result, but there is no application of rules that will ascribe a type to these examples, signalling that these do not meet our well-behavedness properties. However, we want to know these rules are correct in that they reject every program which goes wrong during execution. This property is called \textit{soundness}, and asserts that the static rules are correct with respect to the dynamic rules. The exact definition depends on the language under consideration, but is often split into two parts called progress and preservation. These are given below for $\calc$.

\begin{theorem}[$\calc$ Preservation]
If $~\vdash e: \tau$ and $e \longrightarrow e'$, then $\vdash e': \tau$ for some $e'$.
\end{theorem}

Preservation states that a well-typed term is still well-typed after it has been reduced. This means a sequence of reductions will produce intermediate terms that are also well-typed and do not get stuck. In $\calc$, the type of the term after reduction is the same as the type of the term before reduction.

\begin{theorem}[$\calc$ Progress]
If $~\vdash e: \tau$ and $e$ is not a value, then $e \longrightarrow e'$ for some $e'$.
\end{theorem}

Progress states that any well-typed, non-value term can be reduced i.e. it will not get stuck due to type errors. A consequence of this is that values in the grammar are exactly the well-typed, irreducible expressions. This is intentional and we always define values to be like this. For this reason we will often refer to irreducible expressions as values, even before we have shown they are equivalent.

By combining progress and preservation, we know that a runtime type-error can never occur as the result of a single-step reduction. This is soundness for small-step reductions. Once this has been established, we may extend this to multi-step reductions by inducting on the length of the multi-step and appealing to the soundness of single-step reductions, which yields the following theorem.

\begin{theorem}[$\calc$ Soundness]
If $~\vdash e: \tau$ and $e \longrightarrow^{*} e'$ then $\vdash e': \tau$.
\end{theorem}

All these theorems are proven by structural induction on the typing rule $\Gamma \vdash e: \tau$ used in the premise and, where appropriate, on the reduction rule $e \longrightarrow e'$ used.

There are two common lemmas needed in the proof of soundness. The first is canonical forms, which outlines a set of useful observations that follow immediately from the typing rules. The second is the substitution lemma, which says if a term is well-typed in a context $\Gamma, x: \tau' \vdash e: \tau$, and you replace variable $x$ with an expression $e'$ of type $\tau$, then $\Gamma \vdash [e'/x]e: \tau$. Note how $e$ and $[e'/x]$ are ascribed the same type in the same context. In $\calc$, this lemma is needed to show that the reduction step in \textsc{E-Let2} is type-preserving.

Precise formulations of these lemmas for $\calc$ is given below.

\begin{lemma}[Canonical Forms]
The following are true:
\begin{itemize}
	\setlength\itemsep{-0.7em}
	\item If $\Gamma \vdash v: \Int$, then $v = l$ is a $\Nat$ constant.
	\item If $\Gamma \vdash v: \Bool$, then $b = l$ is a $\Bool$ constant.
\end{itemize}
\end{lemma}

\begin{lemma}[Substitution]
If $\Gamma, x: \tau' \vdash e: \tau$ and $\Gamma \vdash e': \tau'$ then $\Gamma \vdash [e'/x]e:  \tau$.
\end{lemma}

To summarise, soundness establishes that the static rules of a language are correct with respect to its semantics. The converse of soundness is also interesting to consider: if a program has no runtime type error, will the type system accept it? This is called \textit{completeness}. Few type systems are complete, including $\calc$. This means $\calc$ might reject type safe programs. To show why, consider the Java program in Figure \ref{ref:java_typing_completeness}. This program is type-safe, because the only branch of the conditional which ever executes is the one which returns an $\kwa{int}$. However, Java will reject this program because, in general, statically determining which branches can or cannot execute is undecidable.

\begin{figure}[h]
\vspace{-5pt}

\begin{lstlisting}
public int doubleNum(int x) {
   if (true) return x + x;
   else return true;
}
\end{lstlisting}
 
\vspace{-12pt}
\caption{A type-safe Java method which does not typecheck.}
\label{ref:java_typing_completeness}
\end{figure}

This report is only ever concerned with proving soundness, but it is impotrant to recognise that being incomplete makes a type system inherently \textit{conservative}, meaning it can reject type-safe programs or make over-estimations as to what will happen. One view of type systems is that they ``calculate a kind of static  approximation to the run-time behaviours of the terms in a program'' \cite[p. 2]{tapl}. In order to approximate, simplifying assumptions must be made, and these simplifying assumptions are what make the type-system sound; but assumptions which are too generalising may result in more and more type safe examples getting rejected.



\section{ $\stlc$: Simply-Typed $\lambda$-Calculus}

The simply-typed $\lambda$-calculus $\stlc$ is a model of computation, first described by Alonzo Church \cite{church40}, based on the definition and application of functions. $\stlc$ serves as the basis for many programming languages, including those in this report. We present a variant with natural and integer numbers, so we can familiarise ourselves with subtyping. A grammar for $\stlc$ programs is given in Figure \ref{fig:stlc_grammar}.

\begin{figure}[h]
\vspace{-5pt}

\[
\begin{array}{lll}

\begin{array}{lllr}

e & ::= & ~ & exprs: \\
	& | & x & variable \\
	& | & e~e & application \\
	& | & v & value \\
	&&\\
	
\end{array}

\begin{array}{lllr}

v & ::= & ~ & values: \\
	& | & \lambda x: \tau . e & abstraction \\
	& | & n & \Nat~constant \\
	& | & i & \Int~constant \\
	&&\\
	
\end{array}

\end{array}
\]

\vspace{-12pt}
\caption{Grammar for $\stlc$.}
\label{fig:stlc_grammar}
\end{figure}

An expression in $\stlc$ is either a variable $x$, the application of a function to a value $e~e$, or a value. A value can be a $\Nat$ constant $n$, an $\Int$ constant $n$, or the function literal $\lambda x: \tau.e$. To distinguish $\Nat$ constants from positive $\Int$ constants, we write $3_{\mathbb{N}}$ for the former and $3_{\mathbb{Z}}$ for the latter. This is not part of the grammar, it is just our notation for distinguishing between the two categories. In the function literal $\lambda x: \tau.e$, $e$ is the function body, $x$ is the name of the argument to the function, and $\tau$ is the type of the argument. An example is $\lambda x: \Int. ~x$, which is the identity function on integers. $(\lambda x: \Int.~ x)~3_{\mathbb{Z}}$ is the application of that identity function to the integer literal $3_{\mathbb{Z}}$. 

A grammar for types in $\stlc$ is given in Figure \ref{fig:stlc_type_grammar}. A type context $\Gamma$ is a sequence of variable bindings, interpreted in the usual way. There are two base types $\Nat$ and $\Int$. The arrow $\rightarrow$ is a type constructor: it can be used to build a new type from existing ones. $\tau_1 \rightarrow \tau_2$ is the type of a function which takes as input a $\tau_1$ and returns a $\tau_2$. For example, the function $\lambda x: \Int. ~x$ would have the type $\Int \rightarrow \Int$. Some other examples of types are $\Int \rightarrow \Nat$, $\Nat \rightarrow (\Int \rightarrow \Nat)$, and $(\Nat \rightarrow \Nat) \rightarrow (\Nat \rightarrow \Int)$.

\begin{figure}[h]
\vspace{-5pt}

\[
\begin{array}{lll}


\begin{array}{lllr}

\Gamma & ::= & ~ & type~: \\
	& | & \Nat & natural~numbers \\
	& | & \Int & integers \\
	& | & \tau \rightarrow \tau & arrow \\
	&&\\
	
\end{array}


\begin{array}{lllr}

\Gamma & ::= & ~ & type~ctx.: \\
	& | & \varnothing & empty~ctx. \\
	& | & \Gamma, x: \tau & binding \\
	&&\\
	
\end{array}

\end{array}
\]

\vspace{-12pt}
\caption{Grammar for $\stlc$.}
\label{fig:stlc_type_grammar}
\end{figure}

Before giving the small-step semantics, we need to define $\kwa{substitution}$. Its definition is given in Figure \ref{fig:stlc_sub_defn}. Numeric constants are unchanged by substitution. A variable is changed if it matches the variable being replaced. A function has the free variables in its body replaced. An application has the free variables in its subexpressions replaced.

\begin{figure}[h]

\bm{$\kwa{substitution :: e \times e \times v \rightarrow e}$}

\begin{itemize}
	\setlength\itemsep{-0.7em}
	\item[] $[v/y]i = i$
	\item[] $[v/y]n = n$
	\item[] $[ v/y]x =  v$, if $x = y$
	\item[] $[ v/y]x = x$, if $x \neq y$
	\item[] $[ v/y](\lambda x:  \tau.  e) = \lambda x:  \tau.[ v/y] e$, if $y \neq x$ and $y$ does not occur free in $ e$
	\item[] $[ v/y]( e_1~ e_2) = ([ v/y] e_1)([ v/y] e_2)$
\end{itemize}

\vspace{-12pt}
\caption{Substitution for $\stlc$.}
\label{fig:stlc_sub_defn}
\end{figure}

The dynamic rules for $\stlc$ are summarised in Figure \ref{fig:stlc_dynamic_rules}. The only reducible expression is a function application. \textsc{E-App1} will reduce the left-side of an application. If the left-side is a value, but the right-side is an expression, then \textsc{E-App2} will reduce the right-side. If the left side is a function and the right-side is a value, then the right-side is bound to the name of the function's formal argument in the function body. For example, $(\lambda x:\Int.~x)~3_{\mathbb{Z}} \longrightarrow 3_{\mathbb{Z}}$ by \textsc{E-App3}.

\begin{figure}[h]

\noindent
\fbox{$e \longrightarrow e$}

\[
\begin{array}{c}

\infer[\textsc{(E-App1)}]
	{ e_1  e_2 \longrightarrow  e_1'  e_2~|~\varepsilon}
	{ e_1 \longrightarrow  e_1'~|~\varepsilon}
	~~~
\infer[\textsc{(E-App2)}]
	{ v_1  e_2 \longrightarrow  v_1  e_2'~|~\varepsilon} 
	{ e_2 \longrightarrow  e_2'~|~\varepsilon}\\[2ex]
	
\infer[\textsc{(E-App3)}]
	{ (\lambda x:  \tau. e)  v_2 \longrightarrow [ v_2/x] e~|~\varnothing }
	{}\\[2ex]
	
\end{array}
\]

\vspace{-12pt}
\caption{Dynamic rules for $\stlc$.}
\label{fig:stlc_dynamic_rules}
\end{figure}

As with $\calc$, some expressions in $\stlc$ exhibit strange behaviours due to type errors or undefined variables. For example, consider $e = (\lambda x: \Int.~x) (\lambda x:\Int.~x)$. Then $e \longrightarrow e$ by \textsc{E-App3}. This expression can be endlessly reduced! But intuitively, we want to exclude it as a well-behaved program, because the function on the left takes an $\Int$ as an argument, and the function on the right is not an $\Int$. Another example is $(\lambda x: \Int.~y)~3_{\mathbb{Z}}$, which reduces to $y$ by \textsc{E-App3} and then gets stuck. This should be excluded because $y$ is undefined. To determine whether a program is well-behaved we can apply the static rules for $\stlc$, summarised in Figure \ref{fig:stlc_static_rules}.


\begin{figure}[h]

\fbox{$\Gamma \vdash e: \tau$}

\[
\begin{array}{c}

\infer[\textsc{(T-Nat)}]
	{\Gamma \vdash n: \Nat}
	{}
	~~~
\infer[\textsc{(T-Int)}]
	{\Gamma \vdash i: \Int}
	{}
~~~
\infer[\textsc{(T-Var)}]
	{\Gamma, x: \tau \vdash x: \tau}
	{} \\[2ex]
	
\infer[\textsc{(T-Abs)}]
	{\Gamma \vdash \lambda x: \tau_1.e : \tau_1 \rightarrow \tau_2}
	{\Gamma, x: \tau_1 \vdash e: \tau_2} 
	~~~
	
\infer[\textsc{(T-App)}]
	{\Gamma \vdash e_1~e_2: \tau_3}
	{\Gamma \vdash e_1: \tau_2 \rightarrow \tau_3 & \Gamma \vdash e_2: \tau_2}

\end{array}
\]


\vspace{-12pt}
\caption{Static rules for $\stlc$.}
\label{fig:stlc_static_rules}
\end{figure}

The first two rules state that a natural number constant can always be typed to $\Nat$, and an integer constant can always be typed to $\Int$. \textsc{T-Var} states that a variable bound in some context can be typed as its binding. \textsc{T-Abs} states that a function can be typed in $\Gamma$ if $\Gamma$ can type the body of the function, when the function's argument has been bound to its formal type. \textsc{T-App} states that an application is well-typed if the left-hand expression reduces to a function of type $\tau_2 \rightarrow \tau_3$ and the right-hand expression has type $\tau_2$. The examples above will now reject: $(\lambda x: \Int.~x)~(\lambda x:\Int.~x)$ does not type because $\vdash \lambda x: \Int.~x : \Int \rightarrow \Int$, but the right-hand side does not have type $\Int$; $(\lambda x: \Int.~y)~3_{\mathbb{Z}}$ does not type because no rule can type $y$ in the context $x: \Int$.

Consider the example $(\lambda x: \Int.~x)~3_{\mathbb{N}}$, where a natural number is passed to the identity function for integers. The rules cannot type this program because the function expects an $\Int$, but in some sense a $\Nat$ is a specific sort of $\Int$, and sometimes it is convenient to treat it as such. We call $\Nat$ a subtype of $\Int$ and write $\Nat <: \Int$ for this judgement. In general, the judgement form $\tau_1 <: \tau_2$ means that values of type $\tau_1$ are also values of type $\tau_2$. We say $\tau_1$ is a more specific type than $\tau_2$, and that $\tau_2$ is a more general type than $\tau_1$. Subtyping judgements for $\stlc$ are given in Figure \ref{fig:stlc_subtyping}. 

\begin{figure}[h]

\fbox{$\tau <: \tau$}

	
\[
\begin{array}{c}

\infer[\textsc{(S-Reflexive)}]
	{\tau <: \tau}
	{}
	~~~
\infer[\textsc{(S-Transitive)}]
	{\tau_1 <: \tau_3}
	{\tau_1 <: \tau_2 & \tau_2 <: \tau_3}\\[2ex]

\infer[\textsc{(S-Nat)}]
	{\Nat <: \Int}
	{}~~~

\infer[\textsc{(S-Arrow)}]
	{\tau_1 \rightarrow \tau_2 <: \tau_1' \rightarrow \tau_2'}
	{\tau_1' <: \tau_1 & \tau_2 <: \tau_2'}


\end{array}
\]

\vspace{-12pt}
\caption{Static rules for $\stlc$.}
\label{fig:stlc_static_rules}
\end{figure}

The rules \textsc{S-Reflexive} and \textsc{S-Transitive} make subtyping a pre-ordering relation on types. \textsc{S-Nat} says that natural numbers are also integers. The most intriguing rule is \textsc{S-Arrow}, which describes when one function is a subtype of another. Notice how the direction of the subtyping relation is flipped for the input types in the premise, whereas the direction is preserved for the output types. The former is called \textit{contravariance} and the latter \textit{covariance}. We say functions are contravariant in their input type and covariant in their output type.

To illustrate why \textsc{S-Arrow} is sensible, consider $\Int \rightarrow \Int$ and $\Nat \rightarrow \Int$. The former could take either an $\Int$ or a $\Nat$ as input (because $\Nat <: \Int$), but the latter can only take a $\Nat$ as input. $\Int \rightarrow \Int$ functions can therefore take more specific inputs than $\Nat \rightarrow \Int$ functions, so $\Int \rightarrow \Int <: \Nat \rightarrow \Int$; the direction of this judgement is reversed from $\Nat <: \Int$, so input type should be contravariant. On the other hand, consider $\Int \rightarrow \Int$ and $\Int \rightarrow \Nat$. The former might return a $\Nat$ or an $\Int$, but the latter can only return a $\Nat$; then it would be safe to treat $\Int \rightarrow \Nat$ functions as $\Int \rightarrow \Int$ functions, because the former only return $\Nat$ values, and the latter is allowed to return $\Nat$ values. However, $\Int \rightarrow \Int$ functions could return an $\Int$ value, so it would not be safe to treat them as a $\Int \rightarrow \Nat$ function, which can only return a $\Nat$ value. Therefore, $\Int \rightarrow \Nat <: \Int \rightarrow \Int$; the direction of this judgement is the same as $\Nat <: \Int$, so the output types should be covariant.

In order to typecheck an example like $\lambda x: \Int.~3_{\mathbb{N}}$, we need a rule which lets us consider $3_{\mathbb{N}}$ as an $\Int$. More generally, we should be able to treat any subtype as one of its supertypes. This is called subsumption; the rule for it is given in Figure \ref{fig:stlc_subsumption}.

\begin{figure}[h]

\fbox{$\tau <: \tau$}

	
\[
\begin{array}{c}

\infer[\textsc{T-Subsume}]
	{\Gamma \vdash e: \tau_2}
	{\Gamma \vdash e: \tau_1 & \tau_1 <: \tau_2 }

\end{array}
\]

\vspace{-12pt}
\caption{The subsumption rule.}
\label{fig:stlc_subsumption}
\end{figure}

The type system will now accept programs like $(\lambda x: \Int.~x)~3_{\mathbb{N}}$. A derivation for $\vdash (\lambda x: \Int.~x)~3_{\mathbb{N}}: \Int$ is given in Figure \ref{fig:subsume_derivation}.

\begin{figure}[h]


    \begin{prooftree*}
    
        \Infer0[\textsc{(T-Var)}]{x: \Int \vdash x: \Int}
        \Infer1[\textsc{(T-Abs)}]{\vdash \lambda x: \Int~x: \Int \rightarrow \Int}
        
        \Infer0[\textsc{(T-Nat)}]{\vdash 3_{\mathbb{N}}: \Nat}
        \Infer0[\textsc{(S-Nat)}]{\Nat <: \Int}
        \Infer2[\textsc{(T-Subsume)}]{\vdash 3_{\mathbb{N}}: \Int}
        
        \Infer2[\textsc{(T-App)}]{(\lambda x: \Int.~x)~3_{\mathbb{N}}: \Int}

 	\end{prooftree*}
 	
\vspace{-12pt}
\caption{A derivation of $\vdash (\lambda x: \Int.~x)~3_{\mathbb{N}}: \Int$.}
\label{fig:subsume_derivation}
\end{figure}
 
The definition of soundness for $\stlc$ is very similar to $\calc$, but in the presence of subtyping, the type after reduction may get more specific than the type before reduction. To illustrate why this might happen, consider $(\lambda x: \Int.~x)~3_{\mathbb{N}}$. Figure \ref{fig:subsume_derivation} derives the judgement $\vdash (\lambda x: \Int.~x)~3_{\mathbb{N}}: \Int$. By \textsc{E-App3}, $(\lambda x: \Int.~x)~3_{\mathbb{N}} \longrightarrow 3_{\mathbb{N}}$. Then by \textsc{T-Nat}, $\vdash 3_{\mathbb{N}}: \Nat$ --- and $\Nat <: \Int$, so the type got more specific. In general, if a function has input type $\tau$ then it could take any argument which is a subtype of $\tau$. Once that argument has been reduced to a value, we can determine exactly which subtype it is. In general, we cannot statically determine the most precise type of an expression.
 
The soundness property for $\stlc$ is given below. Note how $\tau_B <: \tau_A$, whereas in $\calc$, which had no subtyping, $\tau_B = \tau_A$.

\begin{theorem}[$\stlc$ Soundness]
If $\Gamma \vdash e_A: \tau_A$ and $e_A \longrightarrow^* e_B$, then $\Gamma \vdash e_B: \tau_B$, where $\tau_B <: \tau_A$.
\end{theorem}

As a short aside, $\stlc$ (and $\calc$) are \textit{Turing-incomplete}, meaning there are programs which can be written in general-purpose languages that cannot be written in $\stlc$. There are several routine ways to make $\stlc$ as expressive as these general-purpose languages, but because this report is mainly interested in static rules, we leave our languages Turing-incomplete to simplify the formalisms and minimise irrelevant details. Being Turing-complete is essential for a general purpose programming language, but in this report, we are just demonstrating the static rules (which equally apply to Turing-incomplete program), so it is not necessary.

\section{Effect Systems}

We have seen how the static rules of a language allow us to judge whether certain well-behavedness properties hold of a piece of code, relative to a particular typing context. Some of these well-behavedness properties include being well-typed, and defining every variable before it is used. One extension to classical type systems is to incorporate a theory of \textit{effects}. Judgements in a \textit{type-and-effect} system ascribe both a type and an effect to a piece of code; the effect component describes intensional information about the way in which a program executes \cite{nielson99}. To illustrate, we present a simplified version of $\fxtute$ (Side-Effect Analysis), which is a calculus for reasoning about the set of memory cells that are written or read during execution \cite{nielson99}. Properly defining the small-step semantics of $\fxtute$ requires us to cover more concepts which are largely irrelevant for the rest of the report, so we instead give a quick explanation of how they work.

\subsection{$\fxtute$: Side-Effect Analysis}

$\fxtute$ is a lambda calculus with a type-and-effect system for reasoning about what memory cells are affected by computations. It extends $\stlc$ with imperative constructs for creating, accessing, and updating reference variables. Our interest is in determining which cells might be created, accessed, or updated by a piece of code; effects in $\fxtute$ are therefore one of those three operations on a particular cell. A particular memory cell is denoted $\pi$. It can be thought of as drawn from a set of memory cell variables $\Pi$.

A full definition of $\fxtute$ would include its dynamic rules and a formulation and proof of soundness. Our purpose is to demonstrate how static rules can be used to describe what effects take place during a program execution. To this end, we omit a proper treatment of soundness and reduction, instead giving a quick summary.

The grammar for $\fxtute$ programs is given in Figure \ref{fig:fx_tute}. The first new form is $\refnew{\pi}{x}{e}{e}$, which creates a new reference $x$ in the body of $e_2$, with its value initialised to $e_1$, at location $\pi$. $!x$ is used to access the value of the reference $x$. $x := e$ updates the value of $x$ with $e$.

\begin{figure}[h]

\[
\begin{array}{c}

\begin{array}{lllr}

e & ::= & ~ & exprs: \\
	& | & x & variable \\
	& | & e~e & application \\
	& | & \refnew{\pi}{x}{e}{e} & ref.~creation\\
	& | & !x & ref.~access \\
	& | & x := e & ref.~update \\
	& | & v & value \\
	&&\\
	
\end{array}
	
\begin{array}{lllr}


e & ::= & ~ & exprs: \\
	& | & \lambda x: \tau. e & abstraction \\
	& | & b & boolean~literal \\
	& | & n & natural~literal \\
	&&\\
	

\end{array}
	
\end{array}
\]

\vspace{-12pt}
\caption{Grammar for $\fxtute$ expressions.}
\label{fig:fx_tute}
\end{figure}

In $\fxtute$ an effect $\phi$ is the creation, reading, or writing of a reference at a particular location $\pi$. For example, a program with the effect $!\pi$ is one that reads from memory cell $\pi$ during execution; creating a reference at $\pi$ is $\kwa{new}_{\pi}$; updating a reference at $\pi$ is $\kwa{\pi :=}$. A set of effects is denoted $\Phi$. A grammar for effects is given in \ref{fig:fxtute_fx_regions}.

\begin{figure}[h]

\[
\begin{array}{c}

\begin{array}{lllr}

\phi & ::= & ~ & effects: \\
	& | & \kwa{new}_{\pi} & ref.~creation\\
	& | & !\pi & ref.~access \\
	& | & \pi := & ref.~update \\
	&&\\
	
\end{array}
	
\begin{array}{lllr}

\Phi & ::= & ~ & sets~of~effects: \\
	& | & \{ \bar \phi \} \\
	&&\\
	
\end{array}
	
\end{array}
\]

\vspace{-12pt}
\caption{Grammar for effects and regions of $\fxtute$.}
\label{fig:fxtute_fx_regions}
\end{figure}

The runtime has the notion of a \textit{store}, which maps each reference to the value defined in its cell. The store also keeps track of the location at which a reference was created. It can be enlarged and updated during runtime by the creation, access, and updating of references, each of which incurs a runtime effect $\kwa{new}_{\pi}$, $!\pi$, or $\pi :=$ respectively. Both reading and writing to a reference $x$ will return the value of $x$. Executing a program in a store yields a reduced program, the modified version of the store, and the set of effects $\Phi$ which occurred during the execution.

In our presentation, the base types of $\fxtute$ are $\Nat$ and $\Bool$. $\tau_1 \rightarrow_{\Phi} \tau_2$ is the type of a function which takes a $\tau_1$ as input and returns a $\tau_2$ as output. The set $\Phi$ is an upper-bound on the actual effects incurred by the function: if an effect $\phi$ occurs at runtime, then $\phi \in \Phi$, but it is not guaranteed that every effect in $\Phi$ will happen during execution. There is also a new type constructor $\kwa{ref}$. $\reftype{\tau}{\rho}$ is the type of a reference defined in one of the regions in $\rho$, which points to a value of type $\tau$. The grammar for types is given in \ref{fig:fxtute_types}.

\begin{figure}[h]

\[
\begin{array}{c}

\begin{array}{lllr}

\tau & ::= & ~ & types: \\
	& | & \Nat & natural~numbers \\
	& | & \Bool & booleans \\
	& | & \tau \rightarrow \tau & functions \\
	& | & \reftype{\tau}{\pi} & references \\
	&&\\

\end{array}

\begin{array}{lllr}
	
\Gamma & ::= & ~ & contexts: \\
	& | & \varnothing & empty~ctx. \\
	& | & \Gamma, x: \tau & var.~binding \\
	&&\\
\end{array}

\end{array}
\]

\vspace{-12pt}
\caption{Grammar for $\fxtute$ types.}
\label{fig:fxtute_types}
\end{figure}

There is a single judgement in $\fxtute$, which has the form $\Gamma \vdash e: \tau~\kw{with} \Phi$. This can be read as meaning that, in the context $\Gamma$, $e$ terminates yielding a value of type $\tau$, with $\Phi$ as a conservative upper-bound on the effects incurred during execution. If $\phi \in \Phi$, it is not guaranteed to happen at runtime, but if $\phi \notin \Phi$, it cannot happen at runtime. The static rules are summarised in Figure \ref{fig:fxtute_static}.

\begin{figure}[h]

\fbox{$\Gamma \vdash e: \tau~\kw{with} \Phi$}

\[
\begin{array}{c}

\infer[\textsc{(T-Bool)}]
	{\Gamma \vdash b: \Bool ~\kw{with} \varnothing}
	{}
	~~~
\infer[\textsc{(T-Nat)}]
	{\Gamma \vdash n: \Nat ~\kw{with} \varnothing }
	{} \\[2ex]

\infer[\textsc{(T-Var)}]
	{\Gamma, x: \tau \vdash x: \tau~\kw{with} \varnothing}
	{}
	
~~~
	
\infer[\textsc{(T-Abs)}]
	{\Gamma \vdash \lambda x: \tau_1.e : \tau_1 \rightarrow_{\Phi} \tau_2~\kw{with} \varnothing}
	{\Gamma, x: \tau_1 \vdash e: \tau_2~\kw{with} \Phi} \\[2ex]
	
	
\infer[\textsc{(T-App)}]
	{\Gamma \vdash e_1~e_2: \tau_3~\kw{with} \Phi_1 \cup \Phi_2 \cup \Phi_3}
	{\Gamma \vdash e_1: \tau_2 \rightarrow_{\Phi_3} \tau_3~\kw{with} \Phi_1 & \Gamma \vdash e_2: \tau_2~\kw{with} \Phi_2} \\[2ex]

\infer[\textsc{(T-Read)}]
	{\Gamma, x: \reftype{\tau}{\pi} \vdash~!x : \tau~\kw{with} \{ !\pi \}}
	{} \\[2ex]
	
\infer[\textsc{(T-Write)}]
	{ \Gamma, x: \reftype{\tau}{\pi} \vdash x := e : \tau~\kw{with} \Phi \cup \{ \pi:=\} }
	{ \Gamma, x: \reftype{\tau}{\pi} \vdash e: \tau~\kw{with} \Phi } \\[2ex]

\infer[\textsc{(T-New)}]
	{ \Gamma \vdash \refnew{\pi}{x}{e_1}{e_2}: \tau_2~\kw{with} \Phi_1 \cup \Phi_2 \cup \{ \kwa{new}_{\pi} \} }
	{ \Gamma \vdash e_1: \tau_1~\kw{with} \Phi_1 & \Gamma, x: \reftype{\tau_1}{\pi} \vdash e_2: \tau_2~\kw{with} \Phi_2  } \\[2ex]

\end{array}
\]

\vspace{-12pt}
\caption{Static rules for $\fxtute$.}
\label{fig:fxtute_static}
\end{figure}

The first two rules state that in any context, constants have their appropriate type and no effects. The next three rules are analogous to those in $\stlc$, but with effects included. \textsc{T-Var} says that any variable $x$ has the effect $\varnothing$, so long as the context has a binding for $x$. \textsc{T-Abs} says that if the body of the function has the effects $\Phi$, then the function types to $\tau_1 \rightarrow_{\Phi} \tau_2$. \textsc{T-App} says that applying a function incurs the effects of reducing the two subexpressions to values ($\Phi_1$ and $\Phi_2$) and then the effects of applying the function $(\Phi_3)$.

The new typing rules are for manipulating references. \textsc{T-Read} will type $!x$ to the type $\tau$ referenced by $x$. Its effects are statically approximated as the singleton $\{!\pi\}$, where $\pi$ is the location of $x$ in the typing context. \textsc{T-Write} also has the type $\tau$ referenced by $x$, but its effects are both the operation on the reference $\pi :=$, and the result of reducing the expressino being assigned, $\Phi$. \textsc{T-New} is well-typed if the initial expression $e_1$ of $x$ is well-typed, and the same environment with a new binding $x: \kwa{ref}(\tau_1, \pi)$ can type the rest of the code $e_2$. The effects incurred by the $\kwa{new}$ expression are those incurred by reducing the initial expresion ($\Phi_1$) and those incurred by reducing the rest of the code ($\Phi_2$).

The rules of $\fxtute$ now give us the ability to determine which locations in memory are instantiated, modified, or accessed --- and we do not have to execute the program to find out! As an example, consider the program $e = \refnew{l_1}{x}{3}{ x := 5 }$, which initialises a reference at location $l_1$ with $3$, and then updates it to $5$ . This can be typed as $\vdash e: \Nat~\kw{with} \{ l_1 := \}$; a derivation tree is given in Figure \ref{fig:fxtute_tree}.

\begin{figure}[h]


    \begin{prooftree*}
       \Infer0[\textsc{(T-Nat)}]{\vdash 3: \Nat~\kw{with} \varnothing}
       
       \Infer0[\textsc{(T-Nat)}]{x: \reftype{\Nat}{l_1} \vdash 5: \Nat ~\kw{with} \varnothing}
       
       \Infer1[\textsc{(T-Write)}]{x: \reftype{\Nat}{l_1} \vdash x := 5 : \Nat~\kw{with} \{ l_1 := \}}
       
       \Infer2[\textsc{(T-New)}]{\vdash \refnew{l_1}{x}{3}{x := 5}: \Nat~\kw{with} \{ l_1 := \}}
       
 	\end{prooftree*}
 	
\vspace{-12pt}
\caption{Derivation tree for $\refnew{l_1}{x}{3}{ x := 5 }$.}
\label{fig:fxtute_tree}
\end{figure}

Currently, the expressive power of $\fxtute$ is so low that the approximations from the static rules give \textit{exactly} those effects which will be incurred at runtime. In more complex languages the approximations will stop being tight upper-bounds. As an example of why, consider an extended version of $\fxtute$ with conditional expressions. The conditional $\cond{e_1}{e_2}{e_3}$ will evaluate $e_1$ and check if it is $\true$ or $\false$. If $\true$, it executes $e_1$; if $\false$, it executes $e_2$. A rule for conditionals is given in Figure \ref{fig:fxtute_cond_rule}.

\begin{figure}[h]

\fbox{$\Gamma \vdash e: \tau~\kw{with} \Phi$}

\[
\begin{array}{c}

\infer[\textsc{(T-Cond)}]
	{ \Gamma \vdash \cond{e_1}{e_2}{e_3}: \tau~\kw{with} \Phi_1 \cup \Phi_2 \cup \Phi_3 }
	{ \Gamma \vdash e_1: \Bool~\kw{with} \Phi_1 & \Gamma \vdash e_2: \tau~\kw{with} \Phi_2 & \Gamma \vdash e_3: \tau~\kw{with} \Phi_3 }
	
\end{array}
\]

\vspace{-12pt}
\caption{Static rules for $\fxtute$.}
\label{fig:fxtute_static}
\end{figure}

A conditional is well-typed if the guard $e_1$ types to $\Bool$ and the two branches type to the same $\tau$. Its effects are approximated as the effects incured by reducing the guard, and the effects incurred along both branches. Only branch is executed during runtime, but in general it cannot be statically determined which branch will execute. The only safe conclusion to make is to consider both branches as having executed, with respect to the approximated effects.

\section{The Capability Model}

A \textit{capability} is a unique, unforgeable reference, granting its bearer permission to perform some operation \cite{dennis66}. If a piece of code possesses a capability $C$, it is said to have \textit{authority} over it. In the capability model, authority can only proliferate in the following ways \cite{miller06}:

\begin{enumerate}
	\item By the initial set of capabilities passed into the program (initial conditions).
	\item If a function or object is instantiated by its parent, the parent gains a capability for its child (parenthood).
	\item If a function or object is instantiated by a parent, the parent may endow its child with any capabilities it possesses (endowment).
	\item A capability may be transferred via method-calls or function applications (introduction).
\end{enumerate}

The proliferation rules are summarised as: ``only connectivity begets connectivity.'' There are an initial set of primitive capabilities passed into the program at the beginning of execution by the system environment or virtual machine, which grant operations over \textit{resources} in the system environment. For example, a $\kwa{File}$ might grant operations on a particular file in the file system. Often we conflate the primitive capabilities and the system resources they grant access to, referring to both as resources. A capability is either a primitive capability, or a function or object which captures another capability. An example of a non-primitive capability would be a $\kwa{Logger}$ which, possessing a particular $\kwa{File}$, presents a confined subset of operations on it.

These rules restrict how capabilities spread throughout a program, requiring components to be instantiated with the capabilities they request. As a result, the exercise of any authority is explicit. By contrast, the implicit exercise of authority is known as \textit{ambient authority}. If a language disallows ambient authority and only proliferates capabilities in the above ways, it is called \textit{capability-safe}. Figure \ref{java_ambient_authority} demonstrates one way in which authority can be implicitly exercised in Java: a malicious implementation of $\kwa{List.add}$ attempts to overwrite the user's $\kwa{.bashrc}$ file. $\kwa{MyList}$ gains this capability by importing $\kwa{java.io.File}$ and instantiating new instances of a capability for the user's $\kwa{.bashrc}$ file. In a capability-safe language, $\kwa{MyList}$ would have to be given the $\kwa{.bashrc}$ file on start-up from the system environment directly, or by someone that already possesses it. Another way to exercise ambient authority is through global state: if a capability is stored inside a global variable then any component can acess and use its operations without having been explicitly given it. Therefore, capability-safe languages must disallow global state and unrestricted imports.


\begin{figure}[h]

\begin{lstlisting}
import java.io.File;
import java.io.IOException;
import java.util.ArrayList;

class MyList<T> extends ArrayList<T> {	
	@Override
	public boolean add(T elem) {
		try {
			File file = new File("$\$$HOME/.bashrc");
			file.createNewFile();
		} catch (IOException e) {}
		return super.add(elem);
	}	
}
\end{lstlisting}

\begin{lstlisting}
import java.util.List;

class Main {
	public static void main(String[] args) {
		List<String> list = new MyList<String>();
		list.add(``doIt'');
	}
}
\end{lstlisting}

\vspace{-12pt}
\caption{$\kwa{Main}$ exercises ambient authority over a $\kwa{File}$ capability.}
\label{java_ambient_authority}
\end{figure}

Ambient authority is a challenge to the principle of least authority because it makes it impossible to determine from a module's signature what authority is being exercised. From the perspective of $\kwa{Main}$, knowing that $\kwa{MyList.add}$ has a capability for the user's $\kwa{.bashrc}$ file requires one to inspect the source code of $\kwa{.bashrc}$; a necessity at odds with the circumstances that may surround untrusted code and code ownership.

Capability-safe languages usually have first-class modules, meaning objects and modules are treated in a uniform manner. Modules, like objects, must be instantiated, and can be given their capabilities at this point. They are also bound by the same proliferation rules constraining objects, so the constraints of the capability model are preserved across module boundaries. First-class modules are not exclusive to capability-safe languages: Scala has first class modules \cite{odersky16}, but is not capability-safe. Within the capability-safe languages there is considerable variation in style: Smalltalk is a dynamically-typed capability-safe language with first-class modules \cite{bracha10}. Wyvern is a statically-typed capability-safe language \cite{nistor13} with first-class modules \cite{kurilova16}.
