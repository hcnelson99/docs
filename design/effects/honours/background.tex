\chapter{Background}\label{C:background}

In this chapter we cover the necessary concepts and existing work informing this report. First we detail how a programming language and its type system are formally defined, and how to prove the type system is correct. For this purpose, we present a toy language called Expression-Based Language ($\calc$). We then summarise a variant of the simply-typed lambda calculus $\stlc$. $\stlc$ is the basis for many programming languages, including the capability calculus $\epscalc$. $\epscalc$ is also a capability-based language with an effect system. To understand what this means we cover some existing work on effect systems and discuss Miller's capability model.

\section{Formally Defining a Programming Language}

A programming language can be defined by giving three sets of rules: a grammar, which defines syntactically legal terms; dynamic rules, which give the meaning of a program by defining how it is executed; and static rules, which determine whether particular programs satisfy certain well-behavedness properties. When a language has been defined we want to know its static rules are correct with respect to the dynamic rules; we outline how this can be done.

Alongside the explanation of these concepts we present $\calc$, which is a simple, typed language of arithmetic and boolean expressions. It is a language invented in this report for demonstrative purposes. Like every language we cover, it is expression-based, meaning that programs are evaluated to yield a value. $\calc$ is not very mathematically interesting, but defining it gives a concrete example of the general approach throughout this report.

\subsection{Grammar}

The grammar of a language specifies what strings are syntactically legal. A syntactically legal string is called a \textit{term}. It is specified by giving the different categories of terms and the forms which instantiate those categories. The conventions for specifying a grammar are based on standard Backur-Naur form \cite{bnf}. Figure \ref{fig:ebl_grammar} shows a simple grammar describing integer literals and arithmetic expressions on them. In each rule, the metavariables range over the terms of the category for which they are named.

A $\calc$ program is an expression $e$, consisting of variable definitions, constants, and the application of boolean and arithmetic operators. A valid expression is a variable $x$, a constant (such as $3$, $0$, $\true$, or $\false$), the application of an operator $+$ or $\lor$ to two subexpressions, or a binding for a variable in a piece of code ($\kwa{let}$ expression). The following are $\calc$ terms: $x$, $y$, $3$, $3+2$, $\false \lor \true$, $3 \lor \false$, $\true + \false$, $\letxpr{x}{3}{x+1}$.

For some strings the corresponding term is ambiguous: $3 + x + 2$ could be interpreted as the term $3 + (x + 2)$ or as $(3 + x) + 2$. How we parse strings is not relevant, so we shall only consider strings which unambiguously correspond to terms. Brackets are not part of the grammar; we are just using them to disambiguate the strings we write that are meant to represent terms.\\

\begin{figure}[h]

\[
\begin{array}{c}

\begin{array}{lllr}

e & ::= & ~ & exprs: \\
	& | & x & variable \\
	& | & e + e & addition \\
	& | & e \lor e & disjunction \\
	& | & \letxpr{x}{e}{e} & let~expr. \\
	&&\\
	
v & ::= & ~ & values: \\
	& | & l & \Nat~constant \\
	& | & b & \Bool~constant \\
	&&\\

\end{array}

\end{array}
\]

\vspace{-12pt}
\caption{Grammar for $\calc$ expressions.}
\label{fig:ebl_grammar}
\end{figure}


\subsection{Dynamic Rules}

The dynamic rules of a language specify the meaning of terms. There are different approaches, but the one we use is called \textit{small-step semantics}, where the meaning of a program is given by how it is executed. This is defined with a set of \textit{inference rules}, which are a set of premises above a dividing line. If the premises above the line hold, they imply the \textit{judgement} below the line. A particular application of an inference rule to obtain a judgement is a \textit{derivation}. If an inference rule has no premises it is an \textit{axiom}. Figure \ref{fig:ebl_dynamic} gives the dynamic rules for $\calc$, which specify a binary relation $\longrightarrow$ representing a single computational step. Judgements obtained by the dynamic rules are called \textit{reductions}. When $e \longrightarrow e'$ holds, we say that $e$ reduces to $e'$.

\begin{figure}[h]

\noindent
\fbox{$e \longrightarrow e$}

\[
\begin{array}{c}

\infer[\textsc{(E-Add1)}]
	{e_1 + e_2 \longrightarrow e_1' + e_2}
	{e_1 \longrightarrow e_1'}
~~
\infer[\textsc{(E-Add2)}]
	{l_1 + e_2 \longrightarrow l_1 + e_2'}
	{e_2 \longrightarrow e_2'}
~~
\infer[\textsc{(E-Add3)}]
	{l_1 + l_2 \longrightarrow l_3}
	{l_1 + l_2 = l_3} \\[4ex]

\infer[\textsc{(E-Or1)}]
	{e_1 \lor e_2 \longrightarrow e_1' \lor e_2}
	{e_1 \longrightarrow e_1'}
	~~~
\infer[\textsc{(E-Or2)}]
	{\true \lor e_2 \longrightarrow \true}
	{}
	~~~
\infer[\textsc{(E-Or3)}]
	{\false \lor e_2 \longrightarrow e_2}
	{}\\[4ex]
	
\infer[\textsc{(E-Let1)}]
	{\letxpr{x}{e_1}{e_2} \longrightarrow \letxpr{x}{e_1'}{e_2}}
	{e_1 \longrightarrow e_1'}
	~~~
\infer[\textsc{(E-Let2)}]
	{\letxpr{x}{v}{e_2} \longrightarrow [v/x]e_2}
	{}

\end{array}
\]

\vspace{-12pt}
\caption{Single-step reduction rules for $\calc$.}
\label{fig:ebl_dynamic}
\end{figure}

An addition is reduced by first reducing the left-hand side to an irreducible form (\textsc{E-Add1}) and then the right-hand side (\textsc{E-Add2}). If both sides are integer literals, the expression reduces to whatever is the sum of those literals.

According to these rules, a disjunction is reduced by first reducing the left-hand side to an irreducible form (\textsc{E-Or1}). If the left-hand side is the boolean literal $\true$, the expression reduces to $\true$ (because $\true \lor Q = \true$). Otherwise if the left-hand side is the boolean literal $\false$, the expression reduces to the right-hand side $e_2$ (because $\false \lor Q = Q$). This particular formulation encodes short-circuiting behaviour into $\lor$, meaning if the left-hand side is true, the whole expression will evaluate to true without checking the right-hand side.

A $\kwa{let}$ expression is reduced by first reducing the subexpression being bound (\textsc{E-Let1}). If the subexpression is an irreducible form $v_1$, the variable $x$ is substituted for $v_1$ in the body $e_2$ of the $\kwa{let}$ expression (\textsc{E-Let2}). The notation for this is $[v_1/x]e_2$. For example, $\letxpr{x}{1}{x+1} \longrightarrow 1+1$ by \textsc{E-Let2}.

Formally, substitution is a function on expressions. A definition is given in Figure \ref{fig:ebl_sub_defn}. The notation $[e_1/x]e$ is short-hand for $\kwa{substitution}(e, e_1, x)$. For multiple substitutions we use the notation $[e_1/x_1, e_2/x_2] e$ as shorthand for $[e_2/x_2]([e_1/x_1] e)$. Note how the order of the variables has been flipped: the substitutions occur left-to-right, as they are written.

\begin{figure}[h]

\bm{$\kwa{substitution :: e \times e \times v \rightarrow e}$}

\begin{itemize}
	\setlength\itemsep{-0.7em}
	\item[] $[e'/y]l = l$
	\item[] $[e'/y]b = b$ 
	\item[] $[e'/y]x =  v$, if $x = y$
	\item[] $[e'/y]x = x$, if $x \neq y$
	\item[] $[e'/y](e_1 + e_2) = [e'/y]e_1 + [e'/y]e_2$
	\item[] $[e'/y](e_1 \lor e_2) = [e'/y]e_1 \lor [e'/y]e_2$
	\item[] $[e'/y](\letxpr{x}{e_1}{e_2}) = \letxpr{x}{[e'/y]e_1}{[e'/y]e_2}$, if $y \neq x$ and $y$ does not occur free in $e_1$ or $e_2$
\end{itemize}

\vspace{-12pt}
\caption{Substitution for $\calc$.}
\label{fig:ebl_sub_defn}
\end{figure}

A robust definition of the $\kwa{substitution}$ function is surprisingly tricky. Consider the program $\letxpr{x}{1}{(\letxpr{x}{2}{x+z})}$. It contains two different variables with the same name $x$, with the inner one ``shadowing'' the outer one. Neither variable occurs ``free'', because both have been introduced in the body of the program (one for each $\kwa{let}$). Such variables are called bound variables. By contrast, $z$ is a free variable because it has no definition in the program. A robust $\kwa{substitution}$ should not accidentally conflate two different variables with identical names, and it should not do anything to bound variables.

To illustrate the solution, consider $\letxpr{x}{1}{(\letxpr{x}{2}{x+z})}$. In some sense, this is an equivalent program to $\letxpr{x}{1}{(\letxpr{y}{2}{y+z})}$, because the names of variables are arbitrary, so changing them will not change the semantics of the program. Therefore, we can freely and implicitly interchange expressions which are equivalent up to the naming of bound variables. This process is called $\alpha$-conversion \cite[p. 71]{tapl}. Consequently, we shall assume all expressions we work with have been $\alpha$-converted so all variables are uniquely named and play nice with the definition of $\kwa{substitution}$.

Lastly, note how in an expression like $\letxpr{x}{1+1}{x+1}$, $1+1$ would first be reduced to $2$ before the substitution is made on $x+1$. This way of defining dynamic rules --- so that subexpressions are reduced to irreducible forms before they are bound to their names --- is the \textit{call-by-value} strategy. Some languages --- such as Haskell --- are not call-by-value. In this report we only consider call-by-value semantics.

From the single-step reduction relation we define the multi-step reduction relation as a sequence of zero\footnote{We permit multi-step reductions of length zero to be consistent with Pierce, who defines multi-step reduction as a reflexive relation \cite[p. 39]{tapl}.} or more single-steps. This is written $e \longrightarrow^* e'$. For example, if $e_1 \longrightarrow e_2$ and $e_2 \longrightarrow e_3$, then $e_1 \longrightarrow^* e_3$. Figure \ref{fig:ebl_dyn_multistep} defines multi-step reduction in $\calc$.

\begin{figure}[h]

\noindent
\fbox{$e \longrightarrow^{*} e$}

\[
\begin{array}{c}

\infer[\textsc{(E-MultiStep1)}]
	{ e \longrightarrow^{*}  e}
	{}
~~~
\infer[\textsc{(E-MultiStep2)}]
	{ e \longrightarrow^{*}  e'}
	{ e \longrightarrow  e'} \\[3ex]
	
\infer[\textsc{(E-MultiStep3)}]
	{e \longrightarrow^{*}  e''}
	{ e \longrightarrow^{*}  e' &  e' \rightarrow^{*}  e''}
\end{array}
\]
\vspace{-12pt}
\caption{Multi-step reduction rules for $\calc$.}
\label{fig:ebl_dyn_multistep}
\end{figure}

\textsc{E-MultiStep1} says that an expression can always be ``reduced to itself''. \textsc{E-MultiStep2} says that a single-step reduction is also a multi-step reduction. \textsc{E-MultiStep3} says that if a sequence of reductions can reduce $e$ to $e'$, and another sequence can reduce $e'$ to $e''$, then $e \longrightarrow^* e''$ is a valid sequence of reductions.




\subsection{Static Rules}

When attempting to reduce $\calc$ terms you may find you end up with nonsense, or get stuck in a situation where no rule applies due to a typing error. For example, $\false \lor 3 \longrightarrow 3$ by \textsc{E-Or3}, which is nonsense. $(1+1)+\false \longrightarrow 2 + \false$ by \textsc{E-Add1}, but then you are stuck because $+$ should be applied to numbers, and $\false$ is a boolean. Another example is $\kwa{x+1}$, which gets stuck because $x$ is undefined.

We often want to consider those programs which satisfy certain well-behavedness properties. One such property is that of being \textit{well-typed}: if a program is well-typed then during execution it will never get \textit{stuck} due to type errors, like the application of $+$ to booleans. Another is that every variable in a program must be declared before it is used. If a program satisfies these well-behavedness properties, its execution would never get stuck or produce a nonsense answer.

To determine if a program is well-behaved before it is executed we shall add static rules to $\calc$, enriching it with a basic type system that associates a type to well-behaved expressions. If an expression can be given a type then its execution will have no type errors. Our type system will also encode the requirement that variables be defined before they are used. The relevant constructs for the type system are given as a grammar in Figure \ref{fig:calc_types}. There are two types: $\Nat$ and $\Bool$. A typing context $\Gamma$ maps variables to their types; this is needed in a program like $\letxpr{x}{1}{x+1}$, where in typing $x+1$, we need to know the type of $x$.

\begin{figure}[h]

\[
\begin{array}{c}

\begin{array}{lllr}

\tau & ::= & ~ & types: \\
	& | & \kwa{Nat} \\
	& | & \kwa{Bool} \\
	&&\\
	
\Gamma & ::= & ~ & contexts: \\
	& | & \varnothing \\
	& | & \Gamma, x: \tau \\
	&&\\
\end{array}

\end{array}
\]

\vspace{-12pt}
\caption{Grammar for the type system of $\calc$.}
\label{fig:calc_types}
\end{figure}

Figure \ref{fig:ebl_static} presents the static rules of $\calc$. The judgement form is $\Gamma \vdash e: \tau$, which means expression $e$ has type $\tau$ in the context $\Gamma$. When a judgement can be derived from the empty context it is written $\vdash e: \tau$ instead of $\varnothing \vdash e: \tau$.

\begin{figure}[h]

\noindent
\fbox{$\Gamma \vdash e: \tau$}

\[
\begin{array}{c}

\infer[\textsc{(T-Var)}]
	{\Gamma, x: \kwa{Int} \vdash x: \kwa{Int}}
	{}
~~~
\infer[\textsc{(T-Bool)}]
	{\vdash b : \Bool}
	{}
	~~~
\infer[\textsc{(T-Nat)}]
	{\vdash l : \Nat}
	{}\\[2ex]

	~~~
\infer[\textsc{(T-Or)}]
	{\Gamma \vdash e_1 \lor e_2 : \Bool}
	{\Gamma \vdash e_1: \Bool & \Gamma \vdash e_2: \Bool}
	~~~
\infer[\textsc{(T-Add)}]
	{\Gamma \vdash e_1 + e_2 : \Nat}
	{\Gamma \vdash e_1: \Nat & \Gamma \vdash e_2: \Nat} \\[2ex]
	
\infer[\textsc{(T-Let)}]
	{\Gamma \vdash \letxpr{x}{e_1}{e_2} : \tau_2}
	{\Gamma \vdash e_1: \tau_1 & \Gamma, x: \tau_1 \vdash e_2: \tau_2}
	
	
\end{array}
\]

\vspace{-12pt}
\caption{Inference rules for typing arithmetic expressions.}
\label{fig:ebl_static}
\end{figure}

\textsc{T-Bool} and \textsc{T-Nat} are rules which say that constants always type to $\Bool$ or $\Nat$. \textsc{T-Var} says that a variable types to whatever the context binds it to. \textsc{T-Or} types a disjunction if the arguments are both $\Bool$. \textsc{T-Add} types a sum if the arguments are both $\Nat$. The most interesting rule is \textsc{T-Let}, where the context gains a binding for $x$ to type-check the body of the $\kwa{let}$ expression. For example, $\letxpr{x}{1}{x+1}$ typechecks because $x:\Int \vdash x + 1: \Int$; a derivation is given in Figure \ref{fig:ebl_let_tree}. The type of a $\kwa{let}$ expression is the type of its body.

\begin{figure}[h]


    \begin{prooftree*}
        \Infer0[\textsc{(T-Nat)}]{\vdash 1: \Nat}
        
        \Infer0[\textsc{(T-Var)}]{x: \Int \vdash x: \Int}
        \Infer0[\textsc{(T-Nat)}]{x: \Int \vdash 1: \Int }
        \Infer2[\textsc{(T-Add)}]{x: \Int \vdash x + 1: \Int}
        
        \Infer2[\textsc{(T-Let)}]{\vdash \letxpr{x}{1}{x+1}: \Int}
        
 	\end{prooftree*}
 	
\vspace{-12pt}
\caption{Derivation tree for $\letxpr{x}{1}{x+1}$}
\label{fig:ebl_let_tree}
\end{figure}
 
There are some pesky technicalities about typing contexts which need to be addressed. Though we have defined $\Gamma$ syntactically as a sequence of variable-type pairs, we really want to treat it like a set. For example, $x: \kwa{Int}, y: \kwa{Int}$ is really the same thing as $y: \kwa{Int}, x: \kwa{Int}$. By the convention of $\alpha$-renaming, all programs have unique variable names, so no duplicate names will arise in practice. Some other properties we require is that whenever a judgement holds in a context $\Gamma$, it should also hold in any super-context $\Gamma$. For example, $x:\Int \vdash x:\Int$, but it's also true that $x:\Int, y:\Int \vdash x:\Int$. The order of the bindings in $\Gamma$ should not matter. We can ensure these properties with the rules in Figure \ref{fig:ctx_rules}.

\begin{figure}[h]

\noindent
\fbox{$\Gamma \vdash e: \tau$}

\[
\begin{array}{c}

\infer[\textsc{($\Gamma$-Permute)}]
	{\Gamma' \vdash e: \tau}
	{\Gamma \vdash e: \tau & \Gamma'~is~a~permutation~of~\Gamma}
	~~~
\infer[\textsc{($\Gamma$-Widen)}]
	{\Gamma, x: \tau' \vdash e: \tau }
	{\Gamma \vdash e: \tau & x \notin \kwa{dom}(\Gamma)}

	
\end{array}
\]

\vspace{-12pt}
\caption{Structural rules for typing contexts.}
\label{fig:ctx_rules}
\end{figure}

\textsc{$\Gamma$-Permute} says that a judgement holds in $\Gamma$ if it holds in any permutation of $\Gamma$, meaning the order is irrelevant. \textsc{$\Gamma$-Widen} says that any judgement which holds in $\Gamma$ will hold in $\Gamma, x: \tau$, provided $x$ is not already in the domain of $\Gamma$. $\kwa{dom}(\Gamma)$ is the set of variables bound in $\Gamma$; a definition is given in \ref{fig:ctx_dom_defn}. All of this causes typing contexts to behave as we expect, but in practice the notation for contexts and how to manipulate them is so conventional that we shall never both to mention them again. The structural rules of contexts will be automatically and implicitly applied.

\begin{figure}[h]

\bm{$\kwa{dom :: \Gamma \rightarrow \{ x \}}$}

\begin{itemize}
	\setlength\itemsep{-0.7em}
	\item[] $\kwa{dom}(\varnothing) = \varnothing$
	\item[] $\kwa{dom}(\Gamma, x: \tau) = \kwa{dom}(\Gamma) \cup \{ x \}$
\end{itemize}

\vspace{-12pt}
\caption{Definition of $\kwa{dom}$.}
\label{fig:ctx_dom_defn}
\end{figure}

Most languages have a \textit{subtyping} judgement $\tau_1 <: \tau_2$, which means that expressions of type $\tau_1$ are also of type $\tau_2$. $\calc$ has no subtyping rules, but we shall encounter them later.

\subsection{Soundness}

If we tried to apply the static rules to the misbehaved programs from the last section, we would find there is no way to ascribe a type to them. This is good, but we want to know that every such misbehaving program is rejected by the type system. This property is called \textit{soundness}. The exact definition depends on the language under consideration, but is often split into two parts called progress and preservation. These are given below for $\calc$.

\begin{theorem}[$\calc$ Preservation]
If $\vdash e: \tau$ and $e \longrightarrow e'$, then $\vdash e': \tau$.
\end{theorem}

Preservation states that a well-typed term is still well-typed after it has been reduced. This means a sequence of reductions will produce intermediate terms that are also well-typed and do not get stuck. In $\calc$, the type of the term after reduction is the same as the type of the term before reduction.

\begin{theorem}[$\calc$ Progress]
If $\vdash e: \tau$ and $e$ is not a value, then $e \longrightarrow e'$ for some $e'$.
\end{theorem}

Progress states that any well-typed, non-value term can be reduced i.e. it will not get stuck due to type errors. A consequence of this is that values in the grammar are exactly the well-typed, irreducible expressions. This is intentional and we always define values to be like this. For this reason we will often refer to irreducible expressions as values, even before we have shown they are equivalent. Combining progress and preservation, we know that well-typed programs which are not-values can be single-step reduced (i.e. they are not stuck). This is single-step soundness

\begin{theorem}[$\calc$ Single-step Soundness]
If $\vdash e: \tau$ and $e \longrightarrow e'$ then $\vdash e': \tau$, for some $e'$, $\tau$.
\end{theorem}

All these theorems are proved by structural induction on the derivation of $\Gamma \vdash e: \tau$ in the premise and then, where appropriate, on the derivation of $e \longrightarrow e'$ in the premise. Once small-step soundness has been established, multi-step soundness follows by inducting on the length of a multi-step reduction.

\begin{theorem}[$\calc$ Multi-step Soundness]
If $\vdash e: \tau$ and $e \longrightarrow^{*} e'$ then $\vdash e': \tau$, for some $e'$, $\tau$.
\end{theorem}

There are two common lemmas needed in the proof of soundness. The first is canonical forms, which outlines a set of useful observations that follow immediately from the static rules. The second is the substitution lemma, which says if a term $e$ is well-typed in a context $\Gamma, x: \tau'$, and instances of $x$ in $e$ are replaced with an expression $e'$ of type $\tau'$, then the type of $e$ is preserved. In $\calc$, this lemma is needed to show that the reduction step in \textsc{E-Let2} preserves types. Formulations of these lemmas are given below.

\begin{lemma}[Canonical Forms]
The following are true:
\begin{itemize}
	\setlength\itemsep{-0.7em}
	\item If $\Gamma \vdash v: \Int$, then $v = l$ is a $\Nat$ constant.
	\item If $\Gamma \vdash v: \Bool$, then $b = l$ is a $\Bool$ constant.
\end{itemize}
\end{lemma}

\begin{lemma}[Substitution]
If $\Gamma, x: \tau' \vdash e: \tau$ and $\Gamma \vdash e': \tau'$ then $\Gamma \vdash [e'/x]e:  \tau$.
\end{lemma}

Soundness establishes that the static rules reject every program violating their well-behavedness properties. The converse is also interesting to consider: if a program is well-behaved, will the type system accept it? This is called \textit{completeness}. Most interesting type systems are incomplete. To show why, consider the Java program in Figure \ref{ref:java_typing_completeness}. This program is type-safe, because the only branch of the conditional that ever executes is the one which returns an $\kwa{int}$. However, Java will reject this program because the other branch returns a $\kwa{boolean}$ determining which branches will execute is generally undecidable.

\begin{figure}[h]
\vspace{-5pt}

\begin{lstlisting}
public int doubleNum(int x) {
   if (true) return x + x;
   else return true;
}
\end{lstlisting}
 
\vspace{-12pt}
\caption{A type-safe Java method which does not typecheck.}
\label{ref:java_typing_completeness}
\end{figure}

This report is only concerned with proving soundness, but it is important recognise that being incomplete makes a type system inherently \textit{conservative}, meaning it will reject type-safe programs or make static over-estimations. One view of type systems is that they ``calculate a kind of static  approximation to the run-time behaviours of the terms in a program'' \cite[p. 2]{tapl}. In order to approximate, simplifying assumptions must be made, and these simplifying assumptions are what make the type-system sound and decidable; but assumptions which simplify too much will reject more and more safe programs, making the system less useful.


\section{ $\stlc$: Simply-Typed $\lambda$-Calculus}

The simply-typed $\lambda$-calculus $\stlc$ is a model of computation, first described by Alonzo Church \cite{church40}, based on the definition and application of functions. $\stlc$ serves as the basis for many programming languages, including those in this report. We present a variant with natural numbers, integers, and subtyping. Its grammar is given in Figure \ref{fig:stlc_grammar}.

\begin{figure}[h]
\vspace{-5pt}

\[
\begin{array}{lll}

\begin{array}{lllr}

e & ::= & ~ & exprs: \\
	& | & x & variable \\
	& | & e~e & application \\
	& | & v & value \\
	&&\\
	
\end{array}

\begin{array}{lllr}

v & ::= & ~ & values: \\
	& | & \lambda x: \tau . e & abstraction \\
	& | & n & \Nat~constant \\
	& | & i & \Int~constant \\
	&&\\
	
\end{array}

\end{array}
\]

\vspace{-12pt}
\caption{Grammar for $\stlc$.}
\label{fig:stlc_grammar}
\end{figure}

An expression in $\stlc$ is either a variable $x$, the application of a function to a value $e~e$, or a value. A value can be a $\Nat$ constant $n$, an $\Int$ constant $i$, or a function literal $\lambda x: \tau.e$ (also called an abstraction). To distinguish the strings representing $\Nat$ constants from the strings representing positive $\Int$ constants, we write $3_{\mathbb{N}}$ for the former and $3_{\mathbb{Z}}$ for the latter. This is not part of the grammar, it is just notation. A function literal $\lambda x: \tau.e$ has a function body $e$ and a formal argument $x$ with type $\tau$. For example, $\lambda x: \Int. ~x$ is the identity function on integers. $(\lambda x: \Int.~ x)~3_{\mathbb{Z}}$ is the application of that identity function to the integer literal $3_{\mathbb{Z}}$. 

A grammar for types in $\stlc$ is given in Figure \ref{fig:stlc_type_grammar}. A type context $\Gamma$ is a sequence of variable bindings, interpreted in the usual way. There are two base types: $\Nat$ and $\Int$. $\rightarrow$ (``arrow'') is a type constructor: it builds a new type from existing ones. $\tau_1 \rightarrow \tau_2$ is the type of a function which takes as input a $\tau_1$ and returns a $\tau_2$. The function $\lambda x: \Int. ~x$ would have the type $\Int \rightarrow \Int$. Some other examples of types are $\Int \rightarrow \Nat$, $\Nat \rightarrow (\Int \rightarrow \Nat)$, and $(\Nat \rightarrow \Nat) \rightarrow (\Nat \rightarrow \Int)$.

\begin{figure}[h]
\vspace{-5pt}

\[
\begin{array}{lll}


\begin{array}{lllr}

\Gamma & ::= & ~ & type~: \\
	& | & \Nat & natural~numbers \\
	& | & \Int & integers \\
	& | & \tau \rightarrow \tau & arrow \\
	&&\\
	
\end{array}


\begin{array}{lllr}

\Gamma & ::= & ~ & type~ctx.: \\
	& | & \varnothing & empty~ctx. \\
	& | & \Gamma, x: \tau & var.~binding \\
	&&\\
	
\end{array}

\end{array}
\]

\vspace{-12pt}
\caption{Grammar for $\stlc$.}
\label{fig:stlc_type_grammar}
\end{figure}

Before giving the small-step semantics, we define $\kwa{substitution}$ in Figure \ref{fig:stlc_sub_defn}: numeric constants are unchanged by substitution; a variable is changed if it matches the variable being replaced; a function has the free variables in its body replaced; an application has the free variables in its subexpressions replaced.

\begin{figure}[h]

\bm{$\kwa{substitution :: e \times e \times v \rightarrow e}$}

\begin{itemize}
	\setlength\itemsep{-0.7em}
	\item[] $[v/y]i = i$
	\item[] $[v/y]n = n$
	\item[] $[ v/y]x =  v$, if $x = y$
	\item[] $[ v/y]x = x$, if $x \neq y$
	\item[] $[ v/y](\lambda x:  \tau.  e) = \lambda x:  \tau.[ v/y] e$, if $y \neq x$ and $y$ does not occur free in $ e$
	\item[] $[ v/y]( e_1~ e_2) = ([ v/y] e_1)([ v/y] e_2)$
\end{itemize}

\vspace{-12pt}
\caption{Substitution for $\stlc$.}
\label{fig:stlc_sub_defn}
\end{figure}

The small-step semantics for $\stlc$ are summarised in Figure \ref{fig:stlc_dynamic_rules}. Multi-step semantics are defined the same as in $\calc$. The only reducible expression is a function application. \textsc{E-App1} will reduces the left-side of an application. If the left-side is a value, but the right-side is not, then \textsc{E-App2} reduces the right-side. If the left-side is a function and the right-side is a value, then \textsc{E-App3} binds the right-side to the name of the function's formal argument in the function body. For example, $(\lambda x:\Int.~x)~3_{\mathbb{Z}} \longrightarrow 3_{\mathbb{Z}}$ by \textsc{E-App3}.

\begin{figure}[h]

\noindent
\fbox{$e \longrightarrow e$}

\[
\begin{array}{c}

\infer[\textsc{(E-App1)}]
	{ e_1  e_2 \longrightarrow  e_1'  e_2~|~\varepsilon}
	{ e_1 \longrightarrow  e_1'~|~\varepsilon}
	~~~
\infer[\textsc{(E-App2)}]
	{ v_1  e_2 \longrightarrow  v_1  e_2'~|~\varepsilon} 
	{ e_2 \longrightarrow  e_2'~|~\varepsilon}\\[2ex]
	
\infer[\textsc{(E-App3)}]
	{ (\lambda x:  \tau. e)  v_2 \longrightarrow [ v_2/x] e~|~\varnothing }
	{}\\[2ex]
	
\end{array}
\]

\vspace{-12pt}
\caption{Single-step judgement rules for $\stlc$.}
\label{fig:stlc_dynamic_rules}
\end{figure}

As with $\calc$, some expressions in $\stlc$ exhibit strange behaviours due to type errors or undefined variables. For example, consider $e = (\lambda x: \Int.~x) (\lambda x:\Int.~x)$. By \textsc{E-App3}, $e \longrightarrow \lambda x:\Int.~x$. Intuitively we should reject this program on the left because the function on the left takes an $\Int$ as an argument, and the function on the right is not an $\Int$. Another example is $(\lambda x: \Int.~y)~3_{\mathbb{Z}}$, which reduces to $y$ by \textsc{E-App3} and then gets stuck. This should be rejected because $y$ is undefined. To determine whether a program is well-behaved we can apply the static rules for $\stlc$, summarised in Figure \ref{fig:stlc_static_rules}.


\begin{figure}[h]

\fbox{$\Gamma \vdash e: \tau$}

\[
\begin{array}{c}

\infer[\textsc{(T-Nat)}]
	{\Gamma \vdash n: \Nat}
	{}
	~~~
\infer[\textsc{(T-Int)}]
	{\Gamma \vdash i: \Int}
	{}
~~~
\infer[\textsc{(T-Var)}]
	{\Gamma, x: \tau \vdash x: \tau}
	{} \\[2ex]
	
\infer[\textsc{(T-Abs)}]
	{\Gamma \vdash \lambda x: \tau_1.e : \tau_1 \rightarrow \tau_2}
	{\Gamma, x: \tau_1 \vdash e: \tau_2} 
	~~~
	
\infer[\textsc{(T-App)}]
	{\Gamma \vdash e_1~e_2: \tau_3}
	{\Gamma \vdash e_1: \tau_2 \rightarrow \tau_3 & \Gamma \vdash e_2: \tau_2}

\end{array}
\]


\vspace{-12pt}
\caption{Static rules for $\stlc$.}
\label{fig:stlc_static_rules}
\end{figure}

The first two rules state that a natural number constant can always be typed to $\Nat$ and an integer constant can always be typed to $\Int$. \textsc{T-Var} states that a variable bound in some context can be typed as its binding. \textsc{T-Abs} states that a function can be typed in $\Gamma$ if $\Gamma$ can type the body of the function, when the function's argument has been bound to its formal type. \textsc{T-App} states that an application is well-typed if the left-hand expression reduces to a function of type $\tau_2 \rightarrow \tau_3$ and the right-hand expression has type $\tau_2$. The examples above will now reject: $(\lambda x: \Int.~x)~(\lambda x:\Int.~x)$ does not type because $\vdash \lambda x: \Int.~x : \Int \rightarrow \Int$, but the right-hand side does not have type $\Int$; $(\lambda x: \Int.~y)~3_{\mathbb{Z}}$ does not type because no rule can type $y$ in the context $x: \Int$.

Consider the example $(\lambda x: \Int.~x)~3_{\mathbb{N}}$, where a natural number is passed to the identity function for integers. The rules cannot type this program because the function expects an $\Int$, but in some sense a $\Nat$ is a specific sort of $\Int$, and sometimes it is convenient to treat it as such. We call $\Nat$ a subtype of $\Int$ and write $\Nat <: \Int$ for this judgement. In general, the judgement form $\tau_1 <: \tau_2$ means that values of type $\tau_1$ are also values of type $\tau_2$. We call $\tau_1$ more specific and $\tau_2$ less specific. Subtyping judgements for $\stlc$ are given in Figure \ref{fig:stlc_subtyping}. 

\begin{figure}[h]

\fbox{$\tau <: \tau$}

	
\[
\begin{array}{c}

\infer[\textsc{(S-Reflexive)}]
	{\tau <: \tau}
	{}
	~~~
\infer[\textsc{(S-Transitive)}]
	{\tau_1 <: \tau_3}
	{\tau_1 <: \tau_2 & \tau_2 <: \tau_3}\\[2ex]

\infer[\textsc{(S-Nat)}]
	{\Nat <: \Int}
	{}~~~

\infer[\textsc{(S-Arrow)}]
	{\tau_1 \rightarrow \tau_2 <: \tau_1' \rightarrow \tau_2'}
	{\tau_1' <: \tau_1 & \tau_2 <: \tau_2'}


\end{array}
\]

\vspace{-12pt}
\caption{Static rules for $\stlc$.}
\label{fig:stlc_subtyping}
\end{figure}

The rules \textsc{S-Reflexive} and \textsc{S-Transitive} make subtyping a pre-ordering relation on types. Every system with subtyping will have these rules, so we will not write them down again. \textsc{S-Nat} says that natural numbers are also integers. The most intriguing rule is \textsc{S-Arrow}, which describes when one function is a subtype of another. Notice how the direction of the subtyping relation is flipped for the input types in the premise, whereas the direction is preserved for the output types. We say functions are \textit{contravariant} in their input type and \textit{covariant} in their output type.

To illustrate why \textsc{S-Arrow} is sensible, consider $\Int \rightarrow \Int$ and $\Nat \rightarrow \Int$. The former could take either an $\Int$ or a $\Nat$ (because $\Nat <: \Int$) as input, but the latter can only take a $\Nat$ as input. $\Int \rightarrow \Int$ functions can take every input a $\Nat \rightarrow \Int$ can, so $\Int \rightarrow \Int <: \Nat \rightarrow \Int$. The direction of this judgement is reversed from $\Nat <: \Int$, so input type should be contravariant. On the other hand, consider $\Int \rightarrow \Int$ and $\Int \rightarrow \Nat$. The former might return a $\Nat$ or an $\Int$, but the latter can only return a $\Nat$. It would be safe to treat $\Int \rightarrow \Nat$ functions as $\Int \rightarrow \Int$ functions, because the former only return $\Nat$ values, and the latter is allowed to return $\Nat$ values. However, $\Int \rightarrow \Int$ functions could return an $\Int$ value, so it would not be safe to treat them as a $\Int \rightarrow \Nat$ function, which can only return a $\Nat$ value. Therefore, $\Int \rightarrow \Nat <: \Int \rightarrow \Int$; the direction of this judgement is the same as $\Nat <: \Int$, so the output types should be covariant.

In order to typecheck an example like $\lambda x: \Int.~3_{\mathbb{N}}$, we need a rule which lets us consider $3_{\mathbb{N}}$ as an $\Int$. More generally, we should be able to treat any subtype as one of its supertypes. This is called subsumption; the rule for it is given in Figure \ref{fig:stlc_subsumption}. With it, the type system will now accept programs like $(\lambda x: \Int.~x)~3_{\mathbb{N}}$. A derivation for $\vdash (\lambda x: \Int.~x)~3_{\mathbb{N}}: \Int$ is given in Figure \ref{fig:subsume_derivation}.


\begin{figure}[h]

\fbox{$\tau <: \tau$}

	
\[
\begin{array}{c}

\infer[\textsc{T-Subsume}]
	{\Gamma \vdash e: \tau_2}
	{\Gamma \vdash e: \tau_1 & \tau_1 <: \tau_2 }

\end{array}
\]

\vspace{-12pt}
\caption{The subsumption rule.}
\label{fig:stlc_subsumption}
\end{figure}

\begin{figure}[h]


    \begin{prooftree*}
    
        \Infer0[\textsc{(T-Var)}]{x: \Int \vdash x: \Int}
        \Infer1[\textsc{(T-Abs)}]{\vdash \lambda x: \Int~x: \Int \rightarrow \Int}
        
        \Infer0[\textsc{(T-Nat)}]{\vdash 3_{\mathbb{N}}: \Nat}
        \Infer0[\textsc{(S-Nat)}]{\Nat <: \Int}
        \Infer2[\textsc{(T-Subsume)}]{\vdash 3_{\mathbb{N}}: \Int}
        
        \Infer2[\textsc{(T-App)}]{(\lambda x: \Int.~x)~3_{\mathbb{N}}: \Int}

 	\end{prooftree*}
 	
\vspace{-12pt}
\caption{A derivation of $\vdash (\lambda x: \Int.~x)~3_{\mathbb{N}}: \Int$.}
\label{fig:subsume_derivation}
\end{figure}
 
The definition of soundness for $\stlc$ is similar to $\calc$, but in the presence of subtyping, the type can get more specific after reduction. To illustrate how this can happen, consider $(\lambda x: \Int.~x)~3_{\mathbb{N}}$. Figure \ref{fig:subsume_derivation} shows a derivation of $\vdash (\lambda x: \Int.~x)~3_{\mathbb{N}}: \Int$. By \textsc{E-App3}, $(\lambda x: \Int.~x)~3_{\mathbb{N}} \longrightarrow 3_{\mathbb{N}}$. Then by \textsc{T-Nat}, $\vdash 3_{\mathbb{N}}: \Nat$. $\Nat <: \Int$, so the type got more specific. In general, if a function has input type $\tau$ then it could take any argument which is a subtype of $\tau$. Once that argument has been reduced to a value, we can determine exactly which subtype it is.
 
The soundness theorem for $\stlc$ is stated below.

\begin{theorem}[$\stlc$ Multi-step Soundness]
If $\vdash e_A: \tau_A$ and $e_A \longrightarrow^* e_B$, then $\vdash e_B: \tau_B$, where $\tau_B <: \tau_A$, for some $e_B, \tau_B$.
\end{theorem}

As a short aside, $\stlc$ (and $\calc$) are \textit{Turing-incomplete}, meaning there are programs which can be written in general-purpose languages that cannot be written in $\stlc$. There are several routine ways to make $\stlc$ as expressive as general-purpose languages, but because this report is mainly interested in static rules, we leave our languages Turing-incomplete to simplify the formalisms and minimise irrelevant details. Being Turing-complete is essential for a general purpose programming language, but in this report, we are just demonstrating the static rules (which equally apply to Turing-incomplete program), so it is not necessary.

\section{Effect Systems}

We have seen how the static rules of a language allow us to judge whether certain well-behavedness properties hold of a piece of code in a particular typing context. Some of these well-behavedness properties include being well-typed and defining every variable before it is used. One extension to classical type systems is to incorporate a theory of \textit{effects}. Judgements in a \textit{type-and-effect} system associate both a type and a set of effects with well-behaved programs. Effects describe intensional information about the way in which a program executes \cite{nielson99}. To illustrate, we present a simplified version of Side-Effect Analysis ($\fxtute$), which is a calculus for reasoning about which memory cells are written or read during execution \cite{nielson99}. The small-step semantics introduce more concepts which are largely irrelevant for the rest of the report, so a brief summary is given rather than formal dynamic rules.

\subsection{$\fxtute$: Side-Effect Analysis}

$\fxtute$ extends $\stlc$ with imperative constructs by allowing memory cells to be accessed and written via references. Our goal is to determine which references might be accessed, written, or created during execution. An effect in $\fxtute$ is one of these three operations on a particular memory cell. A particular memory cell is denoted $\pi$. It can be thought of as drawn from a set of memory cell variables $\Pi$. The grammar is given in Figure \ref{fig:fx_tute}. The first new form is $\refnew{\pi}{x}{e}{e}$, which creates a new reference $x$ pointing to cell $\pi$ in the body of $e_2$. The contents of $\pi$ are initialised with $e_1$. $!x$ accesses the value at the cell referenced by $x$. $x := e$ updates the contents of the cell referenced by $x$ with $e$.

\begin{figure}[h]

\[
\begin{array}{c}

\begin{array}{lllr}

e & ::= & ~ & exprs: \\
	& | & x & variable \\
	& | & e~e & application \\
	& | & \refnew{\pi}{x}{e}{e} & ref.~creation\\
	& | & !x & ref.~access \\
	& | & x := e & ref.~update \\
	& | & v & value \\
	&&\\
	
\end{array}
	
\begin{array}{lllr}


v & ::= & ~ & values: \\
	& | & \lambda x: \tau. e & abstraction \\
	& | & n & natural~literal \\
	& | & b & boolean~literal \\
	&&\\
	

\end{array}
	
\end{array}
\]

\vspace{-12pt}
\caption{Grammar for $\fxtute$ expressions.}
\label{fig:fx_tute}
\end{figure}

An effect $\phi$ is the creation, reading, or writing of a reference to a particular location $\pi$. For example, a program with the effect $!\pi$ is one that reads from memory cell $\pi$ during execution. Creating a reference at $\pi$ is $\kwa{new}_{\pi}$. Updating a reference at $\pi$ is $\pi \kwa{:=}$. A set of effects is denoted $\Phi$. A grammar for effects is given in Figure \ref{fig:fxtute_fx_regions}.

\begin{figure}[h]

\[
\begin{array}{c}

\begin{array}{lllr}

\phi & ::= & ~ & effects: \\
	& | & \kwa{new}_{\pi} & ref.~creation\\
	& | & !\pi & ref.~access \\
	& | & \pi := & ref.~update \\
	&&\\
	
\end{array}
	
\begin{array}{lllr}

\Phi & ::= & ~ & sets~of~effects: \\
	& | & \{ \bar \phi \} \\
	&&\\
	
\end{array}
	
\end{array}
\]

\vspace{-12pt}
\caption{Grammar for effects and regions of $\fxtute$.}
\label{fig:fxtute_fx_regions}
\end{figure}

The runtime has the notion of a \textit{store}, which maps references to the values in their cell. The store also keeps track of what location each reference points to. It can be enlarged and updated during runtime by creating, accessing, and updating references. Each of these will incur the appropriate runtime effect. Reading and writing on a reference $x$ both evaluate to the (potentially modified) value in the cell to which $x$ refers. Executing a program in a store yields a reduced program, the modified version of the store, and the set of effects $\Phi$ which occurred during the execution. For brevity, we omit the formal definition of these rules.

The only base type is $\Nat$. $\tau_1 \rightarrow_{\Phi} \tau_2$ is the type of a function which takes a $\tau_1$ as input and returns a $\tau_2$ as output. The set $\Phi$ is an upper-bound on the actual effects incurred by the function: if an effect $\phi$ occurs at runtime, then $\phi \in \Phi$, but it is not guaranteed that every $\phi \in \Phi$ will happen. There is also a new type constructor $\kwa{ref}$. $\reftype{\tau}{\pi}$ is the type of a reference that points to the cell $\pi$, which contains a value of type $\tau$. The grammar for types is given in \ref{fig:fxtute_types}.

\begin{figure}[h]

\[
\begin{array}{c}

\begin{array}{lllr}

\tau & ::= & ~ & types: \\
	& | & \Nat & natural~numbers \\
	& | & \Bool & booleans \\
	& | & \tau \rightarrow \tau & functions \\
	& | & \reftype{\tau}{\pi} & references \\
	&&\\

\end{array}

\begin{array}{lllr}
	
\Gamma & ::= & ~ & contexts: \\
	& | & \varnothing & empty~ctx. \\
	& | & \Gamma, x: \tau & var.~binding \\
	&&\\
\end{array}

\end{array}
\]

\vspace{-12pt}
\caption{Grammar for $\fxtute$ types.}
\label{fig:fxtute_types}
\end{figure}

There is a single judgement $\Gamma \vdash e: \tau~\kw{with} \Phi$. This can be read as meaning that, in the context $\Gamma$, $e$ terminates yielding a value of type $\tau$, with $\Phi$ as a conservative upper-bound on the effects incurred during execution. If $\phi \in \Phi$ it is not guaranteed to happen at runtime, but if $\phi \notin \Phi$, it cannot happen at runtime. The static rules are summarised in Figure \ref{fig:fxtute_static}.

\begin{figure}[h]

\fbox{$\Gamma \vdash e: \tau~\kw{with} \Phi$}

\[
\begin{array}{c}

\infer[\textsc{(T-Bool)}]
	{\Gamma \vdash b: \Bool ~\kw{with} \varnothing}
	{}
	~~~
\infer[\textsc{(T-Nat)}]
	{\Gamma \vdash n: \Nat ~\kw{with} \varnothing }
	{} \\[2ex]

\infer[\textsc{(T-Var)}]
	{\Gamma, x: \tau \vdash x: \tau~\kw{with} \varnothing}
	{}
	
~~~
	
\infer[\textsc{(T-Abs)}]
	{\Gamma \vdash \lambda x: \tau_1.e : \tau_1 \rightarrow_{\Phi} \tau_2~\kw{with} \varnothing}
	{\Gamma, x: \tau_1 \vdash e: \tau_2~\kw{with} \Phi} \\[2ex]
	
	
\infer[\textsc{(T-App)}]
	{\Gamma \vdash e_1~e_2: \tau_3~\kw{with} \Phi_1 \cup \Phi_2 \cup \Phi_3}
	{\Gamma \vdash e_1: \tau_2 \rightarrow_{\Phi_3} \tau_3~\kw{with} \Phi_1 & \Gamma \vdash e_2: \tau_2~\kw{with} \Phi_2} \\[2ex]

\infer[\textsc{(T-Read)}]
	{\Gamma, x: \reftype{\tau}{\pi} \vdash~!x : \tau~\kw{with} \{ !\pi \}}
	{} \\[2ex]
	
\infer[\textsc{(T-Write)}]
	{ \Gamma, x: \reftype{\tau}{\pi} \vdash x := e : \tau~\kw{with} \Phi \cup \{ \pi:=\} }
	{ \Gamma, x: \reftype{\tau}{\pi} \vdash e: \tau~\kw{with} \Phi } \\[2ex]

\infer[\textsc{(T-New)}]
	{ \Gamma \vdash \refnew{\pi}{x}{e_1}{e_2}: \tau_2~\kw{with} \Phi_1 \cup \Phi_2 \cup \{ \kwa{new}_{\pi} \} }
	{ \Gamma \vdash e_1: \tau_1~\kw{with} \Phi_1 & \Gamma, x: \reftype{\tau_1}{\pi} \vdash e_2: \tau_2~\kw{with} \Phi_2  } \\[2ex]

\end{array}
\]

\vspace{-12pt}
\caption{Static rules for $\fxtute$.}
\label{fig:fxtute_static}
\end{figure}

The first two rules state that in any context, constants have their appropriate type and no effects. The next three rules are analogous to those in $\stlc$, but with effects included. \textsc{T-Var} says that any variable $x$ has the effect $\varnothing$, so long as the context has a binding for $x$. \textsc{T-Abs} says that if the body of the function has the effects $\Phi$, then the function types to $\tau_1 \rightarrow_{\Phi} \tau_2$. \textsc{T-App} says that applying a function incurs the effects of reducing the two subexpressions to values ($\Phi_1$ and $\Phi_2$) and then the effects of applying the function $(\Phi_3)$.

The new rules are for references and their operations. \textsc{T-Read} will type $!x$ to the type $\tau$ of the value in the cell referenced by $x$. Its effects are $\{!\pi\}$, where $\pi$ is the cell that $x$ refers to in the context. \textsc{T-Write} also has the type $\tau$ referenced by $x$, but its effects are both the operation on the reference $\pi :=$ and the result of reducing the expression being assigned ($\Phi$). \textsc{T-New} is well-typed if the initial expression $e_1$ of $x$ is well-typed, and the same environment with a new binding $x: \kwa{ref}(\tau_1, \pi)$ can type the rest of the code $e_2$. The effects incurred by the $\kwa{new}$ expression are those incurred by reducing the initial expresion ($\Phi_1$) and those incurred by reducing the rest of the code ($\Phi_2$).

The rules of $\fxtute$ now give us the ability to determine which locations in memory are instantiated, modified, or accessed --- and we do not have to execute the program to find out! As an example, consider the program $e = \refnew{l_1}{x}{3}{ x := 5 }$, which initialises a reference at location $l_1$ with $3$ and then updates it to $5$. This can be typed as $\vdash e: \Nat~\kw{with} \{ l_1 := \}$. A derivation tree is given in Figure \ref{fig:fxtute_tree}.

\begin{figure}[h]


    \begin{prooftree*}
       \Infer0[\textsc{(T-Nat)}]{\vdash 3: \Nat~\kw{with} \varnothing}
       
       \Infer0[\textsc{(T-Nat)}]{x: \reftype{\Nat}{l_1} \vdash 5: \Nat ~\kw{with} \varnothing}
       
       \Infer1[\textsc{(T-Write)}]{x: \reftype{\Nat}{l_1} \vdash x := 5 : \Nat~\kw{with} \{ l_1 := \}}
       
       \Infer2[\textsc{(T-New)}]{\vdash \refnew{l_1}{x}{3}{x := 5}: \Nat~\kw{with} \{ l_1 := \}}
       
 	\end{prooftree*}
 	
\vspace{-12pt}
\caption{Derivation tree for $\refnew{l_1}{x}{3}{ x := 5 }$.}
\label{fig:fxtute_tree}
\end{figure}

Currently, the expressive power of $\fxtute$ is so low that the approximations from the static rules give \textit{exactly} the effects which will happen at runtime. In more complex languages the approximations will stop being tight upper-bounds. To show how that might happen, consider an extended version of $\fxtute$ with conditional expressions. The conditional $\cond{e_1}{e_2}{e_3}$ will evaluate $e_1$ and check if it is $\true$ or $\false$. If $\true$, it executes $e_1$. If $\false$, it executes $e_2$. A rule for conditionals is given in Figure \ref{fig:fxtute_cond_rule}. A conditional is well-typed if the guard $e_1$ types to $\Bool$ and the two branches type to the same $\tau$. Its effects are approximated as the effects incured by reducing the guard, and the effects incurred along both branches. Only branch is executed during runtime, but in general it cannot be statically determined which branch will execute. The only safe conclusion to make is to consider both branches as having executed, with respect to the approximated effects.


\begin{figure}[h]

\fbox{$\Gamma \vdash e: \tau~\kw{with} \Phi$}

\[
\begin{array}{c}

\infer[\textsc{(T-Cond)}]
	{ \Gamma \vdash \cond{e_1}{e_2}{e_3}: \tau~\kw{with} \Phi_1 \cup \Phi_2 \cup \Phi_3 }
	{ \Gamma \vdash e_1: \Bool~\kw{with} \Phi_1 & \Gamma \vdash e_2: \tau~\kw{with} \Phi_2 & \Gamma \vdash e_3: \tau~\kw{with} \Phi_3 }
	
\end{array}
\]

\vspace{-12pt}
\caption{Static rules for $\fxtute$.}
\label{fig:fxtute_cond_rule}
\end{figure}

\section{The Capability Model}

A \textit{capability} is a unique, unforgeable reference, granting its bearer permission to perform some operation \cite{dennis66}. If a piece of code possesses a capability $C$, it is said to have \textit{authority} over it. In the capability model, authority can only proliferate in the following ways \cite{miller06}:

\begin{enumerate}
	\item By the initial set of capabilities passed into the program (initial conditions).
	\item If a function or object is instantiated by its parent, the parent gains a capability for its child (parenthood).
	\item If a function or object is instantiated by a parent, the parent may endow its child with any capabilities it possesses (endowment).
	\item A capability may be transferred via method-calls or function applications (introduction).
\end{enumerate}

The proliferation rules are summarised as: ``only connectivity begets connectivity.'' Any authority in a program either exists from the beginning from those initial capabilities passed in by the system environment, or derive from previous access. A primitive capability grants operations over \textit{resources} in the system environment. For example, a $\kwa{File}$ might grant operations on a particular file in the file system. We will often conflate primitive capabilities with the system resources they grant operations upon, referring to both as resources. An example of a non-primitive capability might be a $\kwa{Logger}$ which exercises authority over a $\kwa{File}$ and presents a $\kwa{log}$ function which appends to it. If an effect is interpreted as some operation performed upon a system resource, then capabilities encapsulate the source of effects; the only way to incur an effect is to possess a capability for it.

If a component uses a capability which it has not been explicitly given, it is exercising \textit{ambient authority}. Figure \ref{java_ambient_authority} demonstrates ambient authority in Java: a malicious implementation of $\kwa{List.add}$ attempts to overwrite the user's $\kwa{.bashrc}$ file. $\kwa{MyList}$ gains a capability for this operation by importing $\kwa{java.io.File}$ and instantiating new instances of a capability for the user's $\kwa{.bashrc}$ file. Nobody has explicitly given it a capability for the $\kwa{.bashrc}$ file; it simply creating one for itself. Another way to exercise ambient authority is through global state: if a capability is stored inside a global variable then any component can use it, regardless of whether it had been given to them. Ambient authority is a challenge to the principle of least authority because it makes it impossible to determine from a module's signature what authority is being exercised. From the perspective of $\kwa{Main}$, knowing that $\kwa{MyList.add}$ has a capability for the user's $\kwa{.bashrc}$ file requires one to inspect the source code of $\kwa{.bashrc}$. In a large code-base, this is tedious and error-prone. If the source code is obfuscated, it is also frustrating. If the developer does not have access to the source code, it is impossible! A language in which authority is explicit and only proliferates according to the capability model is called \textit{capability-safe}. To be capability-safe, a language must not allow unrestricted imports or global state. The result is that every component must select and be instantiated with the authority it needs to function. The implementation of $\kwa{MyList}$ has been rewritten in Figure \ref{java_cap_safe} so it exercises authority over $\kwa{File}$ in an explicit manner.

\begin{figure}

\begin{lstlisting}
import java.io.File;
import java.io.IOException;
import java.util.ArrayList;

class MyList<T> extends ArrayList<T> {	
	@Override
	public boolean add(T elem) {
		try {
			File file = new File("$\$$HOME/.bashrc");
			file.createNewFile();
		} catch (IOException e) {}
		return super.add(elem);
	}	
}
\end{lstlisting}

\begin{lstlisting}
import java.util.List;

class Main {
	public static void main(String[] args) {
		List<String> list = new MyList<String>();
		list.add(``doIt'');
	}
}
\end{lstlisting}

\vspace{-12pt}
\caption{$\kwa{Main}$ exercises ambient authority over a $\kwa{File}$ capability.}
\label{java_ambient_authority}
\end{figure}

Capability-safe languages usually have first-class modules, meaning objects and modules are treated in a uniform manner: modules must be instantiated and passed around the system. Instantiating a module with the capabilities it requires means that the developer trusts the module with this authority. Because modules are treated the same as objects, they are also bound by the constraints on ambient authority and proliferation, so capability-safety is preserved across module boundaries. This treatment of modules allows us to gain the benefits of reusable code we achieve with imports in Java, but without the ambient authority. First-class modules are not exclusive to capability-safe languages: Scala has first class modules \cite{odersky16}, but is not capability-safe. Within the capability-safe languages there is a variation in style and design: Newspeak is dynamically-typed capability-safe language with first-class modules \cite{bracha10} and Wyvern is a statically-typed capability-safe language \cite{nistor13} with first-class modules \cite{kurilova16}.


\begin{figure}[t]

\begin{lstlisting}
import java.io.File;
import java.io.IOException;
import java.util.ArrayList;

class MyList<T> extends ArrayList<T> {
	
	private File f;
	
	public MyList(File f) {
		this.f = f;
	}
	
	@Override
	public boolean add(T elem) {
		try {
			f.createNewFile();
		} catch (IOException e) {}
		return super.add(elem);
	}
	
}
\end{lstlisting}

\vspace{-12pt}
\caption{$\kwa{MyList}$ now exercises explicit authority over the $\kwa{File}$.}
\label{java_cap_Safe}
\end{figure}

