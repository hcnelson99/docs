\chapter{Background}\label{C:background}

In this section we cover some of the necessary concepts and existing work informing this report.

\section{Formally Defining a Programming Language}

A programming language can be defined formally by supplying three sets of rules: the grammar, which defines syntactically legal terms; the static rules, which determine whether programs meet certain well-formedness properties; and the dynamic rules, which express the meaning of a program by defining how they are executed. When a language has been defined we want to know its rules are correct, in the sense that we have defined them.

We illustrate these concepts by presenting $\calc$, which is a simple, type-safe, expression-based language. Although $\calc$ is not very interesting, the process of defining it and proving its rules correct gives us a good idea of the general approach this report will take towards formally specifying programming languages.

After introducing $\calc$, we summarise the rules and properties of a variation of the simply-typed lambda calculus $\stlc$. The $\stlc$ is an historically important model of computation and serves as a widespread basis for many functional programming languages. It is also the basis of $\epscalc$, the key subject of this report, and so a preliminary understanding of $\stlc$ will help us understand $\epscalc$.

\subsection{$\calc$: Expression-Based Language}

$\calc$ is a simple language consisting of the definition of variables and the application of boolean and arithmetic operations. Like every language in this report, it is expression-based, meaning that valid programs can always be evaluated to yield a value.

The grammar of a language specifies what strings are syntactically legal. It is specified by giving the different categories of terms, and specifying all the possible forms which instantiate that category. Metavariables range over the terms of the category for which they are named. The conventions for specifying a grammar are based on standard Backur-Naur form \cite{bnf}. Figure 2.1. shows a simple grammar describing integer literals and arithmetic expressions on them. A syntactically valid string is called a term.

$\calc$ has two types: $\Nat$ and $\Bool$. A context is a mapping from variables to types, defined recursively as either the empty context $\varnothing$, or as a context followed by a single binding. A program in the language consists of an expression $e$. A valid expression is either a variable, a constant (such as $3$, $0$, $\true$, or $\false$), by joining two other valid expressions using $+$ or $\lor$, or by introducing a binding for a variable in a piece of code ($\kwa{let}$ expression). The following are examples of syntactically legal programs: $x$, $y$, $3$, $3+2$, $\false \lor \true$, $3 \lor \false$, $\true + \false$, $\letxpr{x}{3}{x+1}$, $3+(x+2)$.

A string like $3 + (x + 2)$ should be seen as a short-hand for the corresponding abstract syntax tree (AST), whose structure is given by the rules of the grammar. \textbf{A diagram might be nice here}. Sometimes the AST is ambiguous, as in $3 + x + 2$ which might be parsed as $3 + (x + 2)$ or as $(3 + x) + 2$. How we parse and disambiguate is largely an implementation detail, so throughout this report we shall only consider strings which unambiguously correspond to a valid AST.\\

\begin{figure}[h]

\[
\begin{array}{c}

\begin{array}{lllr}

e & ::= & ~ & exprs: \\
	& | & x & variable \\
	& | & e + e & addition \\
	& | & e \lor e & disjunction \\
	& | & \letxpr{x}{e}{e} & let~expr. \\
	&&\\
	
v & ::= & ~ & values: \\
	& | & l & \Nat~constant \\
	& | & b & \Bool~constant \\
	&&\\

\end{array}

\begin{array}{lllr}

\tau & ::= & ~ & types: \\
	& | & \kwa{Nat} \\
	& | & \kwa{Bool} \\
	&&\\
	
\Gamma & ::= & ~ & contexts: \\
	& | & \varnothing \\
	& | & \Gamma, x: \tau \\
	&&\\

\end{array}

\end{array}
\]

\vspace{-12pt}
\caption{Grammar for arithmetic expressions.}
\label{A sample. }
\end{figure}

In designing a language we often want to consider those syntactically legal terms satisfying certain \textit{well-behavedness} properties. One such property is that of being \textit{well-typed}. If a program is well-typed then during execution it will never get \textit{stuck} due to type-errors. For example, when executing $3 + (\kwa{Bool} + 2)$, this program would get stuck when trying to evaluate $\kwa{Bool} + 2$, because addition should be defined on two numeric expressions. Although $3 + (\kwa{Bool} + 2)$ is a syntactically legal program, it is not well-typed. Ideally we want rules to help us determine if this is the case without having to execute the program. Another useful well-formedness property says that every variable used in a program must be declared beforehand.

The static rules of a language give a means of determining which syntactically legal terms satisfy particular properties. They are specified by giving \textit{inference rules}. An inference rule is given as a set of premises above a dividing line which, if they hold, imply the result below the line. If an inference rule has no premises, it is called an \textit{axiom}. An application of an inference rule is called a \textit{judgement}. Judgements often take place in typing contexts ($\Gamma$-terms in our arithmetic language), which map variables to types. Conventionally, we say a term is well-typed in a particular context with the notation $\Gamma \vdash e: \tau$, which means that executing $e$ will result in a term of type $\tau$ (if it terminates), and it will never get stuck due to type-errors. The rules enforcing what is well-typed constitute a language's \textit{type system}.

Although our language has no subtyping, most interesting languages do. This judgement is usually written in the form $\tau_1 <: \tau_2$ and it means that values of $\tau_1$ may be provided anywhere instances of $\tau_2$ are expected, and the program will still be well-typed. A useful principle in the design and understanding of subtyping rules is Liskov's substitution principle, which states that if $\tau_1 <: \tau_2$, then instances of $\tau_2$ can be replaced with instances of $\tau_1$ without changing the program's semantic properties \cite{liskov87}. Subtyping rules are not usually totally semantic-preserving, but we'll occasionally use this ideal to motivate why certain rules are sensible.\\


\begin{figure}[h]

\noindent
\fbox{$\Gamma \vdash e: \tau$}

\[
\begin{array}{c}

\infer[\textsc{(T-Var)}]
	{\Gamma, x: \kwa{Int} \vdash x: \kwa{Int}}
	{}
~~~
\infer[\textsc{(T-Bool)}]
	{\vdash b : \Bool}
	{}
	~~~
\infer[\textsc{(T-Nat)}]
	{\vdash l : \Nat}
	{}\\[2ex]

	~~~
\infer[\textsc{(T-Or)}]
	{\Gamma \vdash e_1 \lor e_2 : \Bool}
	{\Gamma \vdash e_1: \Bool & \Gamma \vdash e_2: \Bool}
	~~~
\infer[\textsc{(T-Add)}]
	{\Gamma \vdash e_1 + e_2 : \Nat}
	{\Gamma \vdash e_1: \Nat & \Gamma \vdash e_2: \Nat} \\[2ex]
	
\infer[\textsc{(T-Let)}]
	{\Gamma \vdash \letxpr{x}{e_1}{e_2} : \tau_2}
	{\Gamma \vdash e_1: \tau_1 & \Gamma, x: \tau_1 \vdash e_2: \tau_2}
	
	
\end{array}
\]

\vspace{-12pt}
\caption{Inference rules for typing arithmetic expressions.}
\label{A sample. }
\end{figure}

There are some pesky technicalities about typing contexts which need to be addressed. Althogh we have defined $\Gamma$ as a \textit{sequence} of variable-type mappings, the order shouldn't really be significant: $x: \kwa{Int}, y: \kwa{Int}$ is really the same thing as $y: \kwa{Int}, x: \kwa{Int}$. Formally, we would specify their equivalence by giving structural rules which allow us to freely permute a context's bindings. Another convention is that any judgement which holds in a context $\Gamma$ should hold in any bigger context $\Gamma'$, where $\Gamma \subseteq \Gamma'$. In practice, the notation for contexts and the rules for how to manipulate them are so conventional that we will not bother supplying them, except for the quick summary in Figure 2.3.

One last convention is that when a judgement can be derived from the empty context, we write $\vdash e: \tau$ instead of $\varnothing \vdash e: \tau$.\\


\begin{figure}[h]

\noindent
\fbox{$\Gamma \vdash e: \tau$}

\[
\begin{array}{c}

\infer[\textsc{($\Gamma$-Permute)}]
	{\Gamma' \vdash e: \tau}
	{\Gamma \vdash e: \tau & \Gamma'~is~a~permutation~of~\Gamma}
	~~~
\infer[\textsc{($\Gamma$-Widen)}]
	{\Gamma, x: \tau' \vdash e: \tau }
	{\Gamma \vdash e: \tau & x \notin \Gamma}

	
\end{array}
\]

\vspace{-12pt}
\caption{Structural rules for typing contexts.}
\label{A sample. }
\end{figure}

The dynamic semantics specifies the meaning of syntactically-valid terms. There are different styles of dynamic semantics, but the one we use is called \textit{small-step}. This involves giving a set of inference rules which specify how a program is executed. A single application of one of these rules is called a \textit{reduction}. If a non-variable expression is irreducible under the dynamic rules, it is called a value. The grammar of $\calc$ also specifies a category of terms called ``value''. As we shall see, these two definitions correspond.

\begin{figure}[h]

\noindent
\fbox{$e \longrightarrow e$}

\[
\begin{array}{c}

\infer[\textsc{(E-Add1)}]
	{e_1 + e_2 \longrightarrow e_1' + e_2}
	{e_1 \longrightarrow e_1'}
~~
\infer[\textsc{(E-Add2)}]
	{l_1 + e_2 \longrightarrow l_1 + e_2'}
	{e_2 \longrightarrow e_2'}
~~
\infer[\textsc{(E-Add3)}]
	{l_1 + l_2 \longrightarrow l_3}
	{l_1 + l_2 = l_3} \\[4ex]

\infer[\textsc{(E-Or1)}]
	{e_1 \lor e_2 \longrightarrow e_1' \lor e_2}
	{e_1 \longrightarrow e_1'}
	~~~
\infer[\textsc{(E-Or2)}]
	{\true \lor e_2 \longrightarrow \true}
	{}
	~~~
\infer[\textsc{(E-Or3)}]
	{\false \lor e_2 \longrightarrow e_2}
	{}\\[4ex]
	
\infer[\textsc{(E-Let1)}]
	{\letxpr{x}{e_1}{e_2} \longrightarrow \letxpr{x}{e_1'}{e_2}}
	{e_1 \longrightarrow e_1'}
	~~~
\infer[\textsc{(E-Let2)}]
	{\letxpr{x}{v}{e_2} \longrightarrow [v/x]e_2}
	{}

\end{array}
\]


\vspace{-12pt}
\caption{Inference rules for single-step reductions.}
\label{A sample. }
\end{figure}

Figure 2.4. gives the rules for $\calc$ specifying a single-step reduction relation. The judgement $e \longrightarrow e'$ is a single computational step. In these rules, a disjunction is reduced by first reducing the left-hand side to a value (\textsc{E-Or1}). If the left-hand side is the boolean literal $\true$, then we can reduce the expression to $\true$ (because $\true \lor Q = \true$). Otherwise if the left-hand side is the boolean literal $\false$, we can reduce the expression to the right-hand side $e_2$ (because $\false \lor Q = Q$). This particular formulation of the rules encodes short-circuiting behaviour into $\lor$, meaning that if the left-hand side is true, the expression evaluates to true without checking the right-hand side. If you try to apply the reduction rules to $\calc$ terms which are not well-typed, you can get strange results. For example, $\false \lor 3 \rightarrow 3$ via \textsc{E-Or3}. Throughout this report we are only ever going to be interested in executing programs which are well-typed.

An addition expression is reduced by first reducing the left-hand side to a value (\textsc{E-Add1}) and then the right-hand side (\textsc{E-Add2}) to a value. When both sides are integer literals, the expression reduces to whatever is the sum of those literals.

A let expression is reduced by first reducing the subexpression being bound (\textsc{E-Let1}). When that is a value, we substitute the variable $x$ for the value $v_1$ in the body $e_2$ of the $\kwa{let}$ expression. The notation for this is $[v_1/x]e_2$. This strategy of reducing expressions is \textit{call-by-value}. In a call-by-value strategy, expressions being bound in a program are reduced to values before they are bound to their formal name ($e_1$ is reduced to $v_1$ before it is bound to $x$ in the body $e_2$). Figure 2.5. gives a precise definition of substitution.


\begin{figure}[h]

\bm{$\kwa{substitution :: e \times e \times v \rightarrow e}$}

\begin{itemize}
	\setlength\itemsep{-0.7em}
	\item[] $[e'/y]l = l$
	\item[] $[e'/y]b = b$ 
	\item[] $[e'/y]x =  v$, if $x = y$
	\item[] $[e'/y]x = x$, if $x \neq y$
	\item[] $[e'/y](e_1 + e_2) = [e'/y]e_1 + [e'/y]e_2$
	\item[] $[e'/y](e_1 \lor e_2) = [e'/y]e_1 \lor [e'/y]e_2$
	\item[] $[e'/y](\letxpr{x}{e_1}{e_2}) = \letxpr{x}{[e'/y]e_1}{[e'/y]e_2}$, if $y \neq x$ and $y$ does not occur free in $e_1$ or $e_2$
\end{itemize}

\vspace{-12pt}
\caption{Substitution for $\calc$.}
\label{This is the label.}
\end{figure}

The notation $[e_1/x]e$ is short-hand for $\kwa{substitution}(e, e_1, x)$. When performing multiple substitutions we use the notation $[e_1/x_1, e_2/x_2] e$ as shorthand for $[e_2/x_2]([e_1/x_1] e)$. Note how the order of the variables has been flipped; the substitutions occur as they are written, left-to-right.

A robust definition of the $\kwa{substitution}$ function is surprisingly tricky due to issues surrounding accidental variable capture. Consider the program $\letxpr{x}{1}{(\letxpr{x}{2}{x+z})}$. It contains two different variables with the same name $x$, with the inner one ``shadowing'' the outer one. Neither variable occurs ``free'', because both have been introduced in the body of the program (one for each $\kwa{let}$). Such variables are called bound variables. By contrast, $z$ is a free variable because it has no definition in the program. A robust $\kwa{substitution}$ should not accidentally conflate two different variables with identical names, and it should not do anything to bound variables.

To avoid these issues we adopt the convention of $\alpha$-conversion \cite[p. 71]{tapl}. To illustrate, consider $\letxpr{x}{1}{(\letxpr{x}{2}{x+z})}$. In some sense, this is an equivalent program to $\letxpr{x}{1}{(\letxpr{y}{2}{y+z})}$. Because the names of variables are arbitrary, changing them will not change the semantics of the program. Therefore, we freely and implicitly interchange expressions which are equivalent up to the naming of bound variables, in order to elide some tedious bookkeeping. This process is called $\alpha$-conversion. Consequently, we assume variables are (re-)named in this way to avoid these problems and to play nicely with the definition of $\kwa{substitution}$. \\

Given a single-step reduction relation, we may define a multi-step reduction relation as a sequence of zero\footnote{We permit multi-step reductions of length zero to be consistent with Pierce, who defines multi-step reduction as a reflexive relation \cite[p. 39]{tapl}.} or more single-steps. This is written $e \longrightarrow^* e'$. For example, if $e_1 \longrightarrow e_2 \longrightarrow e_3$, we may also write $e_1 \longrightarrow^* e_3$. 

\begin{figure}[h]

\noindent
\fbox{$e \longrightarrow^{*} e$}

\[
\begin{array}{c}

\infer[\textsc{(E-MultiStep1)}]
	{ e \longrightarrow^{*}  e}
	{}
~~~
\infer[\textsc{(E-MultiStep2)}]
	{ e \longrightarrow^{*}  e'}
	{ e \longrightarrow  e'} \\[3ex]
	
\infer[\textsc{(E-MultiStep3)}]
	{e \longrightarrow^{*}  e''}
	{ e \longrightarrow^{*}  e' &  e' \rightarrow^{*}  e''}
\end{array}
\]
\vspace{-12pt}
\caption{Dynamic rules.}
\label{This is the label.}
\end{figure}

Almost all type systems in which we are interested are \textit{sound}. Soundness is a property that holds between the static and dynamic rules of a language. It says that if a program $e$ is considered well-typed by the static rules, then its reduction under the dynamic rules will never produce a runtime type-error. It is a guarantee that our typing judgements, as we intuitively understand them, are mathematically correct. The exact definition of soundness depends on the language under consideration, but is often split into two parts called progress and preservation.

\begin{theorem}[Progress]
If $\Gamma \vdash e: \tau$ and $e$ is not a value, then $e \longrightarrow e'$.
\end{theorem}

Progress states that any non-value term can be reduced. This essentially says that the definition of a value as a non-variable expression which cannot be reduced under the reduction rules, is coincident with the definition of a value as a particular category of terms in the grammar.

\begin{theorem}[Preservation]
If $\Gamma \vdash e: \tau$ and $e \longrightarrow e'$ then $\Gamma \vdash e': \tau$.
\end{theorem}

Preservation states that a well-typed term is still well-typed after it has been reduced. This implies that a term cannot get stuck during its reduction, because a sequence of reductions produces intermediate terms that are also well-typed. Note that in this particular system the type of the term after reduction is the same as the type of the term before reduction.

By combining progress and preservation, we know that a runtime type-error can never occur as the result of a single-step reduction. Once this has been established, we may extend this to multi-step reductions by inducting on the length of the multi-step and appealing to the soundness of single-step reductions. This yields the following result.

\begin{theorem}[Soundness]
If $\Gamma \vdash e: \tau$ and $e \longrightarrow e'$ then $\Gamma \vdash e': \tau$.
\end{theorem}

These theorems are proven by structural induction on the typing rule used $\Gamma \vdash e: \tau$ or on the reduction rule used $e \rightarrow e'$. Let $T$ be the property to show for all judgements. For each possible inference rule that might have used, you show $T$ holds. Your base cases will be to show $T$ holds for all axioms. Your inductive cases will be to show the theorem holds for non-axiomatic inference rules, when $T$ is assumed to hold for their sub-derivations.s Together, this implies any axiom will have property $T$, and any application of an inference rule based on premises with property $T$ will yield a result with property $T$, showing every provable judgement has property $T$.

In order to prove certain cases of progress and preservation there are two common lemmas needed. The first is canonical forms, which outlines a set of observations that follow immediately by observing the typing rules. The second is the substitution lemma, which says if a term is well-typed in a context $\Gamma, x: \tau' \vdash e: \tau$, and you replace variable $x$ with an expression $e'$ of type $\tau$, then $\Gamma \vdash [e'/x]e: \tau$. In $\calc$, this lemma is needed to show that the reduction step in \textsc{E-Let2} preserves soundness. The other languages considered in this report will have similar reduction steps.

A precise formulation of these two lemmas for $\calc$ is given below.

\begin{lemma}[Canonical Forms]
The following are true:
\begin{itemize}
	\setlength\itemsep{-0.7em}
	\item If $\Gamma \vdash v: \Int$, then $v = l$ is a $\Nat$ constant.
	\item If $\Gamma \vdash v: \Bool$, then $b = l$ is a $\Bool$ constant.
\end{itemize}
\end{lemma}


\begin{lemma}[Substitution]
If $\Gamma, x: \tau' \vdash e: \tau$ and $\Gamma \vdash e': \tau'$ then $\Gamma \vdash [e'/x]e:  \tau$.
\end{lemma}

Proofs for these lemmas and theorems can be found in Appendix A.\\

To summarise, soundness is a property which says, generally, that if a type system says a program is well-typed, it will not encounter a runtime type-error. The corollary of this is also interesting to consider: if a program has no runtime type-error, will the type system accept it? This property is called \textit{completeness}, and almost no (interesting) type-systems are complete. This means a type system may reject type-safe programs. However, soundness guarantees that a type system will \textit{always} reject programs which are \textit{not} type-safe. Consider Figure 2.7., which demonstrates a type-safe Java program rejected by Java's type-system: the body of $\kwa{double}$ is type-safe, because the conditional will always execute $\kwa{return x+x}$. However, Java will reject this program.

\begin{figure}[h]
\vspace{-5pt}

\begin{lstlisting}
public int double(int x) {
   if (true) return x + x;
   else return true;
}
\end{lstlisting}
 
\vspace{-12pt}
\caption{A well-typed Java method which does not typecheck.}
\label{This is the label.}
\end{figure}

Throughout this paper we will only be concerned with sound type systems, but it is important to recognise that these type systems are all \textit{conservative} because they may reject type-safe programs. One view of type-systems is that they ``calculate a kind of static  approximation to the run-time behaviours of the terms in a program'' \cite[p. 2]{tapl}. In order to approximate, simplifying assumptions must be made, and these simplifying assumptions are what make the type-system sound; but assumptions which are too generalising can make the system too conservative and of less practical use. This is an important trade-off we will discuss in motivating certain aspects of $\epscalc$.



\subsection{ $\stlc$: Simply-Typed $\lambda$-Calculus}

The simply-typed $\lambda$-calculus $\stlc$ is a model of computation, first described by Alonzo Church \cite{church40}, based on the definition and application of functions. In this section we present a variation of $\stlc$ with subtyping and summarise its basic properties. Various $\lambda$-calculi serve as the basis for numerous functional programming languages, including $\epscalc$. This section gives us an opportunity to familiarise ourself with $\stlc$ to help introduce $\epscalc$.

\begin{figure}[h]
\vspace{-5pt}

\[
\begin{array}{lll}

\begin{array}{lllr}

e & ::= & ~ & exprs: \\
	& | & x & variable \\
	& | & e~e & application \\
	& | & v & value \\
	&&\\
	
v & ::= & ~ & values: \\
	& | & \lambda x: \tau . e & abstraction \\
	&&\\
	
\end{array}

& ~~~~~~ &

\begin{array}{lllr}

\tau & ::= & ~ & types: \\
	& | & B & base~type \\
	& | & \tau \rightarrow \tau & arrow~type \\
	&&\\
	
\Gamma & ::= & ~ & contexts: \\
	& | & \varnothing & empty~ctx. \\
	& | & \Gamma, x: \tau & var.~binding \\
	&&\\
	
\end{array}

\end{array}
\]

\vspace{-12pt}
\caption{Grammar for $\stlc$.}
\label{This is the label.}
\end{figure}

Types in $\stlc$ are either drawn from a set of base types B, or constructed using $\rightarrow$ (``arrow''). Given types $\tau_1$ and $\tau_2$, $\rightarrow$ can be used to compose a new type, $\tau_1 \rightarrow \tau_2$, which is the type of function taking $\tau_1$-typed terms as input to produce $\tau_2$-typed terms as output. For example, given $B = \{ \Bool, \Int \}$, the following are examples of valid types: $\Bool$, $\Int$, $\Bool \rightarrow \Bool$, $\Bool \rightarrow \Int$, $\Bool \rightarrow (\Bool \rightarrow \Int)$. Arrow is right-associative, so $\Bool \rightarrow \Bool \rightarrow \Int = \Bool \rightarrow (\Bool \rightarrow \Int)$. ``Arrow-type'' and ``function-type'' will be used interchangeably.

In addition to variables, there are function definitions (``abstraction'') and the application of a function to an expression (``application''). For example, $\lambda x: \Int . x$ is the identity function on integers. $(\lambda x: \Int . x) 3$ is the application of the identity function to the integer literal $3$. $(\lambda x: \Int . x) \true$ is the applciation of the identity function to a boolean literal, which is syntactically valid, but as we'll see is not well-typed. A more drastic example is $\true~3$, which is trying to apply $\true$ to $3$. Again, this is a syntactically valid term, but not well-typed because $\true$ is not a function.\\





\begin{figure}[h]

\fbox{$\Gamma \vdash e: \tau$}

\[
\begin{array}{c}


\infer[\textsc{(T-Var)}]
	{\Gamma, x: \tau \vdash x: \tau}
	{}
	
~~~
	
\infer[\textsc{(T-Abs)}]
	{\Gamma \vdash \lambda x: \tau_1.e : \tau_1 \rightarrow \tau_2}
	{\Gamma, x: \tau_1 \vdash e: \tau_2} \\[2ex]
	
	
\infer[\textsc{(T-App)}]
	{\Gamma \vdash e_1~e_2: \tau_3}
	{\Gamma \vdash e_1: \tau_2 \rightarrow \tau_3 & \Gamma \vdash e_2: \tau_2}
	~~~
\infer[\textsc{T-Subsume}]
	{\Gamma \vdash e: \tau_2}
	{\Gamma \vdash e: \tau_1 & \tau_1 <: \tau_2 }

\end{array}
\]

	
\fbox{$\tau <: \tau$}

	
\[
\begin{array}{c}


\infer[\textsc{(S-Arrow)}]
	{\tau_1 \rightarrow \tau_2 <: \tau_1' \rightarrow \tau_2'}
	{\tau_1' <: \tau_1 & \tau_2 <: \tau_2'}\\[2ex]


\end{array}
\]

\vspace{-12pt}
\caption{Static rules for $\stlc$.}
\label{This is the label.}
\end{figure}

Static rules for $\stlc$ are summarised in Figure 2.8. \textsc{T-Var} states that a variable bound in some context can be typed as its binding. \textsc{T-Abs} states that a function can be typed in $\Gamma$ if $\Gamma$ can type the body of the function when the function's argument has been bound. \textsc{T-App} states that an application is well-typed if the left-hand expression is a function (has an arrow-type $\tau_2 \rightarrow \tau_3$) and the right-hand expression has the same type as the function's input ($\tau_2$).

\textsc{T-Subsume} is the rule which says you may a type a term more generally as any of its supertypes. For example, if we had base types $\Int$ and $\Real$, and a rule specifying $\Int <: \Real$, a term of type $\Int$ can also be typed as $\Real$. This allows programs such as $(\lambda x: \Real. x)~3$ to type, as shown in Figure 2.9.

\begin{figure}[h]


    \begin{prooftree*}
        \Infer0[\textsc{(T-Var)}]{x: \Real \vdash x: \Real}
        \Infer1[\textsc{(T-Abs)}]{\vdash \lambda x: \Real . x : \Real \rightarrow \Real }
        
        \Hypo{\vdash 3: \Int}
        \Hypo{\Int <: \Real}
        \Infer2[\textsc{(T-Subsume)}]{\vdash 3: \Real}
        
        \Infer2[\textsc{(T-App)}]{\vdash (\lambda x: \Real . x)~3 : \Real}
 	\end{prooftree*}
 	
\vspace{-12pt}
\caption{Derivation tree showing how \textsc{T-Subsume} can be used.}
\label{This is the label.}
\end{figure}
 


The only subtyping rule we provide is \textsc{S-Arrow}, which describes when one function is a subtype of another. Note how the subtyping relation on the input types is reversed from the subtyping relation on the functions. This is called \textit{contravariance}. Contrast this with the relation on the output type, which preserves the order. That is called \textit{covariance}. Arrow-types are contravariant in their input and covariant in their output.

This presentation has no subtyping rules without premises (axioms), which means there is no way to actually prove a particular subtyping judgement. In practice, we add subtyping axioms for the base-types we have chosen as primitive in our calculus. For example, given base types $\Int$ and $\kwa{Real}$, we might add $\Real <: \Int$ as a rule. This is largely an implementation detail particular to your chosen set of base-types, so we give no subtyping axioms here (but will later when describing $\epscalc$).

\begin{figure}[h]

\bm{$\kwa{substitution :: e \times e \times v \rightarrow e}$}

\begin{itemize}
	\setlength\itemsep{-0.7em}
	\item[] $[ v/y]x =  v$, if $x = y$
	\item[] $[ v/y]x = x$, if $x \neq y$
	\item[] $[ v/y](\lambda x:  \tau.  e) = \lambda x:  \tau.[ v/y] e$, if $y \neq x$ and $y$ does not occur free in $ e$
	\item[] $[ v/y]( e_1~ e_2) = ([ v/y] e_1)([ v/y] e_2)$
\end{itemize}

\vspace{-12pt}
\caption{Substitution for $\stlc$.}
\label{This is the label.}
\end{figure}

Substitution in $\stlc$ follows the same conventions as it does in $\calc$. Substitution on an application is the same as substitution on its sub-expressions. Substitution on a function involves substitution on the function body.

\begin{figure}[h]

\noindent
\fbox{$e \longrightarrow e$}

\[
\begin{array}{c}

\infer[\textsc{(E-App1)}]
	{ e_1  e_2 \longrightarrow  e_1'  e_2~|~\varepsilon}
	{ e_1 \longrightarrow  e_1'~|~\varepsilon}
	~~~
\infer[\textsc{(E-App2)}]
	{ v_1  e_2 \longrightarrow  v_1  e_2'~|~\varepsilon} 
	{ e_2 \longrightarrow  e_2'~|~\varepsilon}\\[2ex]
	
\infer[\textsc{(E-App3)}]
	{ (\lambda x:  \tau. e)  v_2 \longrightarrow [ v_2/x] e~|~\varnothing }
	{}\\[2ex]
	
\end{array}
\]

\vspace{-12pt}
\caption{Dynamic rules for $\stlc$.}
\label{This is the label.}
\end{figure}

Applications are the only reducible expressions in $\stlc$. Such an expression is reduced by first reducing the left subexpression (\textsc{E-App1}). For a well-typed expression, this will always be a function. Once that is a value, the right subexpression is reduced (\textsc{E-App2}). When both subexpressions are values, the right subexpression replaces the formal argument of the function via substitution. The multi-step rules for $\stlc$ are identical to those in $\calc$.

The soundness property for $\stlc$ is as follows.

\begin{theorem}[$\stlc$ Soundness]
If $\Gamma \vdash e_A: \tau_A$ and $e_A \longrightarrow^* e_B$, then $\Gamma \vdash e_B: \tau_B$, where $\tau_B <: \tau_A$.
\end{theorem}

Note how with the inclusion of subtyping rules, the type after reduction can get more specific than the type before reduction, but never less specific. This is in contrast to $\calc$, where the type remains the same.

$\stlc$ is also strongly-normalizing, meaning that well-typed terms always halt i.e. they eventually yield a value. As a consequence it is \textit{not} Turing complete, meaning there are certain computer programs which cannot be written in $\stlc$. By comparison, the \textit{untyped} $\lambda$-calculus is known to be Turing complete \cite{kleene43}. The essential ingredient missing from $\stlc$ is a means of general recursion. In mainstream languages such as Java, this is realised by constructs like the $\kwa{while}$ loop; in the untyped $\lambda$-calculus by the Y-combinator. $\stlc$ can be made Turing-complete by adding a $\kwa{fix}$ operator which mimics the Y-combinator.

Turing-completeness is an essential property for practical, general-purpose programming languages. However, the key contribution of this report is in the static rules of $\epscalc$, and not the expressive power of its dynamic semantics. Therefore we acknowledge this practical short-coming, but leave the basis of $\epscalc$ as a Turing-incomplete language to reduce the number of rules and simplify its presentation.

\textbf{Revisit this depending on how you encode types and stuff in $\epscalc$}


\section{Effect Systems}

In the previous section we looked at how the static rules of a language might make a judgement such as $\Gamma \vdash e: \tau$, which ascribes the type $\tau$ to program $e$. This expresses a certain about what the runtime behaviour of the program is: namely that successive reductions of $e$ will produce terms of type $\tau$, and that this sequence of reductions will never get stuck due to a runtime type-error.

One extension to classical type systems is to incorporate a theory of \textit{effects}. A \textit{type-and-effect} system can ascribe a type and an effect to a piece of code, the effect component of which specifies intensional information about what will happen during the execution of the program \cite{nielson99}. For example, a judgement like $\Gamma \vdash e: \tau~\kw{with} \{ \kwa{File.write} \}$ means that successive reductions of $e$ will result in terms of type $\tau$, and during execution might write to a file. This tells us extra information about what can happen during runtime.

\textbf{Talk about use of effect systems. Mention the basic effect system we will introduce.}

\subsection{ETL: Effect-Typed Language}




\section{The Capability Model}

A \textit{capability} is a unique, unforgeable reference, giving its bearer permission to perform some operation \cite{dennis66}. A piece of code $S$ has \textit{authority} over a capability $C$ if it can directly invoke the operations endowed by $C$; it has \textit{transitive authority} if it can indirectly invoke the operations endowed by a capability $C$ (for example, by deferring to another piece of code with authority over $C$).

In a capability model, authority can only proliferate in the following ways \cite{miller06}:

\begin{enumerate}
	\item By the initial set of capabilities passed into the program (initial conditions).
	\item If a function or object is instantiated by its parent, the parent gains a capability for its child (parenthood).
	\item If a function or object is instantiated by a parent, the parent may endow its child with any capabilities it possesses (endowment).
	\item A capability may be transferred via method-calls or function applications (introduction).
\end{enumerate}

The rules of authority proliferation are summarised as: ``only connectivity begets connectivity''.

Primitive capabilities are called \textit{resources}. Resources model those initial capabilities passed into the runtime from the system environment. A capability is either a resource, or a function or object with (potentially transitive) authority over a capability. An example of a resource might be a particular file. A function which manipulates that file (for example, a logger) would also be a capability, but not a resource. Any piece of code which uses a capability, directly or indirectly, is called \textit{impure}. For example, $\kwa{\lambda x: \kwa{Int} .~x}$ is pure, while $\kwa{\lambda f: File .~f.log(``error~message'')}$ is impure.

A relevant concept in the design of capability-based programming languages is \textit{ambient authority}. This is a kind of exercise of authority over a capability $C$ which has not been explicitly \cite{miller03}. Figure 2.4. gives an example in Java, where a malicious implementation of $\kwa{List.add}$ attempts to overwrite the user's $\kwa{.bashrc}$ file. $\kwa{MyList}$ gains this capability by importing the $\kwa{java.io.File}$ class, but its use of files is not immediate from the signature of its functions.

Ambient authority is a challenge to POLA because it makes it impossible to determine from a module's signature what authority is being exercised. From the perspective of $\kwa{Main}$, knowing that $\kwa{MyList.add}$ has a capability for the user's $\kwa{.bashrc}$ file requires one to inspect the source code of $\kwa{.bashrc}$; a necessity at odds with the circumstances which often surround untrusted code and code ownership.

\begin{figure}[h]

\begin{lstlisting}
import java.io.File;
import java.io.IOException;
import java.util.ArrayList;

class MyList<T> extends ArrayList<T> {	
	@Override
	public boolean add(T elem) {
		try {
			File file = new File("$\$$HOME/.bashrc");
			file.createNewFile();
		} catch (IOException e) {}
		return super.add(elem);
	}	
}
\end{lstlisting}

\begin{lstlisting}
import java.util.List;

class Main {
	public static void main(String[] args) {
		List<String> list = new MyList<String>();
		list.add(``doIt'');
	}
}
\end{lstlisting}

\vspace{-12pt}
\caption{$\kwa{Main}$ exercises ambient authority over a $\kwa{File}$ capability.}
\label{A sample. }
\end{figure}

A language is \textit{capability-safe} if it satisfies this capability model and disallows ambient authority. Some examples include E, Js, and Wyvern. \textbf{Get citations.}

\section{First-Class Modules}

The exact way in which modules work is language-dependent, but we are particularly interested in languages with a first-class module systems. First-class modules are important in capability-safe languages because they mean capability-safe reasoning operates across module boundaries. Because modules are first-class, they must be instantiated like regular objects. They must therefore select their capabilities, and be supplied those capabilities by the proliferation rules of the capability model. In practice, first-class modules can be achieved by having module declarations desugar into an underlying lambda or object representation. This generally requires an ``intermediate representation'' of the language, which is simpler than the one in which programmers write.

Java is an example of a mainstream language whose modules are not first-class. Scala has first-class modules \cite{odersky16}, but is not capability-safe. Smalltalk is a dynamically-typed capability-safe language with first-class modules \cite{bracha10}. Wyvern is a statically-typed capability-safe language with first-class modules \cite{kurilova16}.
