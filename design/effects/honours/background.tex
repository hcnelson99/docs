\chapter{Background}\label{C:background}

In this section we cover some of the necessary concepts and existing work informing this report. First we cover the process of formally defining a programming language and proving some of its properties. For this purpose, we present $\calc$. We then summarise the rules and properties of a variation of the simply-typed lambda calculus $\stlc$. $\stlc$ is an historically important model of computation and serves as a basis for many functional programming language. It is also the basis of $\epscalc$, so a preliminary understanding of $\stlc$ will help us understand $\epscalc$.

$\epscalc$ is a capability-based language with an effect system. To understand what that means we cover some existing work on effect systems and discuss Miller's capability model.


\section{Formally Defining a Programming Language}

A programming language can be defined formally by supplying three sets of rules: a grammar, which defines syntactically legal terms; static rules, which determine whether programs meet certain well-formedness properties; anddynamic rules, which express the meaning of a program by defining how they are executed. When a language has been defined we want to know its rules are mathematically correct.

We illustrate these concepts by presenting $\calc$, which is a simple, type-safe language based around boolean and arithmetic expressions. Like every language in this report, it is expression-based, meaning that valid programs can always be evaluated to yield a value. Although $\calc$ is not very interesting, the process of defining it and proving its rules correct illustrate the general approach this report will take towards formally specifying programming languages.

\subsection{Grammar}

The grammar of a language specifies what strings are syntactically legal. It is specified by giving the different categories of terms, and specifying all the possible forms which instantiate that category. Metavariables range over the terms of the category for which they are named. The conventions for specifying a grammar are based on standard Backur-Naur form \cite{bnf}. Figure 2.1. shows a simple grammar describing integer literals and arithmetic expressions on them. A syntactically valid string is called a term.

A $\calc$ program is an expression $e$, consisting of variable definitions and the application of boolean and arithmetic operations. A valid expression is either a variable, a constant (such as $3$, $0$, $\true$, or $\false$), by joining two other valid expressions using $+$ or $\lor$, or by introducing a binding for a variable in a piece of code ($\kwa{let}$ expression). The following are examples of syntactically legal programs: $x$, $y$, $3$, $3+2$, $\false \lor \true$, $3 \lor \false$, $\true + \false$, $\letxpr{x}{3}{x+1}$, $3+(x+2)$.

A string like $3 + (x + 2)$ should be seen as a short-hand for the corresponding abstract syntax tree (AST), whose structure is given by the rules of the grammar. \textbf{A diagram might be nice here}. Sometimes the AST is ambiguous, as in $3 + x + 2$ which might be parsed as $3 + (x + 2)$ or as $(3 + x) + 2$. How we parse and disambiguate is an implementation detail, so throughout this report we only consider strings which unambiguously correspond to a valid AST.\\

\begin{figure}[h]

\[
\begin{array}{c}

\begin{array}{lllr}

e & ::= & ~ & exprs: \\
	& | & x & variable \\
	& | & e + e & addition \\
	& | & e \lor e & disjunction \\
	& | & \letxpr{x}{e}{e} & let~expr. \\
	&&\\
	
v & ::= & ~ & values: \\
	& | & l & \Nat~constant \\
	& | & b & \Bool~constant \\
	&&\\

\end{array}

\end{array}
\]

\vspace{-12pt}
\caption{Grammar for $\calc$ expressions.}
\label{A sample. }
\end{figure}


\subsection{Dynamic Rules}

The dynamic rules of a language specify the meaning of syntactically-valid terms. There are different approaches, but the one we use is called \textit{small-step semantics}, where the meaning of a program is given by how it is executed. This is specified as a set of \textit{inference rules}. An inference rule is given as a set of premises above a dividing line which, if they hold, imply the result below the line. If an inference rule has no premises it is called an \textit{axiom}. An instantiation of a particular inference rule is called a judgement.

\begin{figure}[h]

\noindent
\fbox{$e \longrightarrow e$}

\[
\begin{array}{c}

\infer[\textsc{(E-Add1)}]
	{e_1 + e_2 \longrightarrow e_1' + e_2}
	{e_1 \longrightarrow e_1'}
~~
\infer[\textsc{(E-Add2)}]
	{l_1 + e_2 \longrightarrow l_1 + e_2'}
	{e_2 \longrightarrow e_2'}
~~
\infer[\textsc{(E-Add3)}]
	{l_1 + l_2 \longrightarrow l_3}
	{l_1 + l_2 = l_3} \\[4ex]

\infer[\textsc{(E-Or1)}]
	{e_1 \lor e_2 \longrightarrow e_1' \lor e_2}
	{e_1 \longrightarrow e_1'}
	~~~
\infer[\textsc{(E-Or2)}]
	{\true \lor e_2 \longrightarrow \true}
	{}
	~~~
\infer[\textsc{(E-Or3)}]
	{\false \lor e_2 \longrightarrow e_2}
	{}\\[4ex]
	
\infer[\textsc{(E-Let1)}]
	{\letxpr{x}{e_1}{e_2} \longrightarrow \letxpr{x}{e_1'}{e_2}}
	{e_1 \longrightarrow e_1'}
	~~~
\infer[\textsc{(E-Let2)}]
	{\letxpr{x}{v}{e_2} \longrightarrow [v/x]e_2}
	{}

\end{array}
\]


\vspace{-12pt}
\caption{Inference rules for single-step reductions.}
\label{A sample. }
\end{figure}

Figure 2.4. gives the dynamic rules for $\calc$. Their conclusions specify members of a binary relation $\longrightarrow$, representing a single computational step. When the relation holds of a particular pair, we say the judgement $e \longrightarrow e'$ holds, and that $e$ reduces to $e'$. 

If a non-variable expression is irreducible under the dynamic rules, it is called a value. The grammar of $\calc$ also specifies a category of terms called ``value''. As we shall see, these two definitions correspond. $3$, $\true$, and $\false$ are examples of irreducible expressions.

A disjunction is reduced by first reducing the left-hand side to a value (\textsc{E-Or1}). If the left-hand side is the boolean literal $\true$, then we can reduce the expression to $\true$ (because $\true \lor Q = \true$). Otherwise if the left-hand side is the boolean literal $\false$, we can reduce the expression to the right-hand side $e_2$ (because $\false \lor Q = Q$). This particular formulation of the rules encodes short-circuiting behaviour into $\lor$, meaning that if the left-hand side is true, the expression evaluates to true without checking the right-hand side.

An addition expression is reduced by first reducing the left-hand side to a value (\textsc{E-Add1}) and then the right-hand side (\textsc{E-Add2}) to a value. When both sides are integer literals, the expression reduces to whatever is the sum of those literals.

A let expression is reduced by first reducing the subexpression being bound (\textsc{E-Let1}). When that is a value, we substitute the variable $x$ for the value $v_1$ in the body $e_2$ of the $\kwa{let}$ expression. The notation for this is $[v_1/x]e_2$. For example, $\letxpr{x}{1}{x+1}$ reduces to $1+1$ by an application of \textsc{E-Let2}.

Consider $\letxpr{x}{1+1}{x+1}$. According to the rules, $1+1$ would first be reduced to $2$ before the substitution is made into $x+1$. This strategy of reducing expressions before they are bound to variable names is \textit{call-by-value}. 

Formally, substitution is a function operating on expressions. A definition is given in Figure 2.5. The notation $[e_1/x]e$ is short-hand for $\kwa{substitution}(e, e_1, x)$. When performing multiple substitutions we use the notation $[e_1/x_1, e_2/x_2] e$ as shorthand for $[e_2/x_2]([e_1/x_1] e)$. Note how the order of the variables has been flipped; the substitutions occur as they are written, left-to-right.

\begin{figure}[h]

\bm{$\kwa{substitution :: e \times e \times v \rightarrow e}$}

\begin{itemize}
	\setlength\itemsep{-0.7em}
	\item[] $[e'/y]l = l$
	\item[] $[e'/y]b = b$ 
	\item[] $[e'/y]x =  v$, if $x = y$
	\item[] $[e'/y]x = x$, if $x \neq y$
	\item[] $[e'/y](e_1 + e_2) = [e'/y]e_1 + [e'/y]e_2$
	\item[] $[e'/y](e_1 \lor e_2) = [e'/y]e_1 \lor [e'/y]e_2$
	\item[] $[e'/y](\letxpr{x}{e_1}{e_2}) = \letxpr{x}{[e'/y]e_1}{[e'/y]e_2}$, if $y \neq x$ and $y$ does not occur free in $e_1$ or $e_2$
\end{itemize}

\vspace{-12pt}
\caption{Substitution for $\calc$.}
\label{This is the label.}
\end{figure}

A robust definition of the $\kwa{substitution}$ function is surprisingly tricky. Consider the program $\letxpr{x}{1}{(\letxpr{x}{2}{x+z})}$. It contains two different variables with the same name $x$, with the inner one ``shadowing'' the outer one. Neither variable occurs ``free'', because both have been introduced in the body of the program (one for each $\kwa{let}$). Such variables are called bound variables. By contrast, $z$ is a free variable because it has no definition in the program. A robust $\kwa{substitution}$ should not accidentally conflate two different variables with identical names, and it should not do anything to bound variables.

To illustrate the solution, consider $\letxpr{x}{1}{(\letxpr{x}{2}{x+z})}$. In some sense, this is an equivalent program to $\letxpr{x}{1}{(\letxpr{y}{2}{y+z})}$. Because the names of variables are arbitrary, changing them will not change the semantics of the program. Therefore, we freely and implicitly interchange expressions which are equivalent up to the naming of bound variables. This process is called $\alpha$-conversion \cite[p. 71]{tapl}. Consequently, we assume variables are (re-)named in this way to avoid these problems and to play nicely with the definition of $\kwa{substitution}$.

Given a single-step reduction relation, we may define a multi-step reduction relation as a sequence of zero\footnote{We permit multi-step reductions of length zero to be consistent with Pierce, who defines multi-step reduction as a reflexive relation \cite[p. 39]{tapl}.} or more single-steps. This is written $e \longrightarrow^* e'$. For example, if $e_1 \longrightarrow e_2$ and $e_2 \longrightarrow e_3$, then $e_1 \longrightarrow^* e_3$. Figure 2.4. shows how multi-step reduction can be defined with a set of inference rules.

\begin{figure}[h]

\noindent
\fbox{$e \longrightarrow^{*} e$}

\[
\begin{array}{c}

\infer[\textsc{(E-MultiStep1)}]
	{ e \longrightarrow^{*}  e}
	{}
~~~
\infer[\textsc{(E-MultiStep2)}]
	{ e \longrightarrow^{*}  e'}
	{ e \longrightarrow  e'} \\[3ex]
	
\infer[\textsc{(E-MultiStep3)}]
	{e \longrightarrow^{*}  e''}
	{ e \longrightarrow^{*}  e' &  e' \rightarrow^{*}  e''}
\end{array}
\]
\vspace{-12pt}
\caption{Dynamic rules.}
\label{This is the label.}
\end{figure}





\subsection{Static Rules}

If you try to reduce some terms you either end up with nonsense or get stuck in a situation where no rule applies. For example, $(1+1)+\false \longrightarrow 2 + \false$ by \textsc{E-Add1}, but then you are stuck. $\false \lor 3 \longrightarrow 3$ by \textsc{E-Or3}, which is strange.

When designing a language we often want to consider those syntactically legal terms satisfying certain \textit{well-behavedness} properties. One such property is that of being \textit{well-typed}: if a program is well-typed then during execution it will never get \textit{stuck} due to type-errors. Another useful well-formedness property says that every variable used in a program must be declared beforehand. The examples above are \textit{not} well-typed, because they are applying operators to arguments of the wrong type. We want rules to help us determine if this is the case without having to execute the program.

The static rules of $\calc$ describe a basic type system which let us determine, without executing a program, whether it contains type errors. The relevant constructs for $\calc$ are given as a grammar in Figure 2.5. There are two types: $\Nat$ and $\Bool$. Furthermore, there is the notion of a \textit{typing context}, which maps variables to their types. This is needed in the case of a program like $\letxpr{x}{1}{x+1}$. In trying to determine whether $x+1$ is well-typed, we need to know what is the type of $x$. To do this, when we see the binding $x=1$ we extend the context to say that $x$ has type $\Nat$.


\begin{figure}[h]

\[
\begin{array}{c}

\begin{array}{lllr}

\tau & ::= & ~ & types: \\
	& | & \kwa{Nat} \\
	& | & \kwa{Bool} \\
	&&\\
	
\Gamma & ::= & ~ & contexts: \\
	& | & \varnothing \\
	& | & \Gamma, x: \tau \\
	&&\\
\end{array}

\end{array}
\]

\vspace{-12pt}
\caption{Grammar for arithmetic expressions.}
\label{A sample. }
\end{figure}

Figure 2.6. summarises the static rules of $\calc$. Note that every judgement holds in a particular typing context. For example, the judgement $x: \Int \vdash x + 1: \Int$ is a claim about a particular property ($x+1$ has the type $\Int$) in a particular context (the context where $x$ is an $\Int$). If a judgement can be derived from the empty context, we conventionally write it as $\vdash e: \tau$ instead of $\varnothing \vdash e: \tau$.

\textsc{T-Bool} and \textsc{T-Nat} are rules which say that constants always type to $\Bool$ or $\Nat$. \textsc{T-Var} says that a variable types to whatever the context binds it to. \textsc{T-Or} types a disjunction if the arguments are both $\Bool$. \textsc{T-Add} types a sum if the arguments are both $\Nat$. The most interesting rule is \textsc{T-Let}, where the context gains a binding for $x$ when type-checking the body of the $\kwa{let}$ expression. The type of a $\kwa{let}$ expression is the type of its body.

\begin{figure}[h]

\noindent
\fbox{$\Gamma \vdash e: \tau$}

\[
\begin{array}{c}

\infer[\textsc{(T-Var)}]
	{\Gamma, x: \kwa{Int} \vdash x: \kwa{Int}}
	{}
~~~
\infer[\textsc{(T-Bool)}]
	{\vdash b : \Bool}
	{}
	~~~
\infer[\textsc{(T-Nat)}]
	{\vdash l : \Nat}
	{}\\[2ex]

	~~~
\infer[\textsc{(T-Or)}]
	{\Gamma \vdash e_1 \lor e_2 : \Bool}
	{\Gamma \vdash e_1: \Bool & \Gamma \vdash e_2: \Bool}
	~~~
\infer[\textsc{(T-Add)}]
	{\Gamma \vdash e_1 + e_2 : \Nat}
	{\Gamma \vdash e_1: \Nat & \Gamma \vdash e_2: \Nat} \\[2ex]
	
\infer[\textsc{(T-Let)}]
	{\Gamma \vdash \letxpr{x}{e_1}{e_2} : \tau_2}
	{\Gamma \vdash e_1: \tau_1 & \Gamma, x: \tau_1 \vdash e_2: \tau_2}
	
	
\end{array}
\]

\vspace{-12pt}
\caption{Inference rules for typing arithmetic expressions.}
\label{A sample. }
\end{figure}

There are some pesky technicalities about typing contexts which need to be addressed. Althogh we have defined $\Gamma$ as a \textit{sequence} of variable-type mappings, the order shouldn't really be significant: $x: \kwa{Int}, y: \kwa{Int}$ is really the same thing as $y: \kwa{Int}, x: \kwa{Int}$. We essentially want to treat it as a set.

Formally, we can specify their equivalence by giving structural rules which say that a judgement holds in $\Gamma$ if it holds in any permutation of $\Gamma$. Another convention is that any judgement which holds in a context $\Gamma$ should hold in any bigger context $\Gamma'$, where $\Gamma \subseteq \Gamma'$. For example, $x: \Int \vdash x: \Int$, but it is also true that $x: \Int, y: \Int \vdash x: \Int$. In practice, the notation for contexts and the rules for how to manipulate them are so conventional that, beyond the quick summary in Figure 2.3., we will not bother to mention them again.

\begin{figure}[h]

\noindent
\fbox{$\Gamma \vdash e: \tau$}

\[
\begin{array}{c}

\infer[\textsc{($\Gamma$-Permute)}]
	{\Gamma' \vdash e: \tau}
	{\Gamma \vdash e: \tau & \Gamma'~is~a~permutation~of~\Gamma}
	~~~
\infer[\textsc{($\Gamma$-Widen)}]
	{\Gamma, x: \tau' \vdash e: \tau }
	{\Gamma \vdash e: \tau & x \notin \Gamma}

	
\end{array}
\]

\vspace{-12pt}
\caption{Structural rules for typing contexts.}
\label{A sample. }
\end{figure}

Though $\calc$ has no subtyping, most interesting languages do. This judgement is written form $\tau_1 <: \tau_2$ and it means that expressions of $\tau_1$ may be provided anywhere expressions of $\tau_2$ are expected, and the program will still be well-typed. A useful principle in the design and understanding of subtyping rules is Liskov's substitution principle, which states that if $\tau_1 <: \tau_2$, then instances of $\tau_2$ can be replaced with instances of $\tau_1$ without changing the program's semantic properties \cite{liskov87}. Subtyping rules are not usually totally semantic-preserving, but we'll occasionally use this idea to justify why certain subtyping rules are sensible.

\subsection{Soundness}

Having defined a type system, we want to know it is \textit{sound}: that if the type system says a program is well-typed, the program will not run into type errors during execution. Soundness is a guarantee that our typing judgements, as we intuitively understand them, are mathematically correct. The exact definition of soundness depends on the language under consideration, but is often split into two parts called progress and preservation.

\begin{theorem}[Progress]
If $~\vdash e: \tau$ and $e$ is not a value, then $e \longrightarrow e'$.
\end{theorem}

Progress states that any well-typed, non-value term can be reduced i.e. it will not get stuck due to type errors. It also says that the definition of a value, as a non-variable irreducible expression, is coincident with the definition of a value as a particular category of terms in the grammar.

\begin{theorem}[Preservation]
If $~\vdash e: \tau$ and $e \longrightarrow e'$ then $\vdash e': \tau$.
\end{theorem}

Preservation states that a well-typed term is still well-typed after it has been reduced. This means a sequence of reductions will produce intermediate terms that are also well-typed and do not get stuck. Note that in this particular formulation of preservation for $\calc$, the type of the term after reduction is the same as the type of the term before reduction.

By combining progress and preservation, we know that a runtime type-error can never occur as the result of a single-step reduction. This is \textit{small-step soundness}. Once this has been established, we may extend this to multi-step reductions by inducting on the length of the multi-step and appealing to the soundness of single-step reductions. This yields the following result.

\begin{theorem}[Soundness]
If $~\vdash e: \tau$ and $e \longrightarrow e'$ then $\vdash e': \tau$.
\end{theorem}

These theorems are proven by structural induction on the typing rule used $\Gamma \vdash e: \tau$ or on the reduction rule used $e \rightarrow e'$. 

In order to prove certain cases of progress and preservation there are two common lemmas needed. The first is canonical forms, which outlines a set of observations that follow immediately by observing the typing rules. The second is the substitution lemma, which says if a term is well-typed in a context $\Gamma, x: \tau' \vdash e: \tau$, and you replace variable $x$ with an expression $e'$ of type $\tau$, then $\Gamma \vdash [e'/x]e: \tau$. In $\calc$, this lemma is needed to show that the reduction step in \textsc{E-Let2} preserves soundness. The other languages considered in this report will have similar reduction steps.

A precise formulation of these two lemmas for $\calc$ is given below.

\begin{lemma}[Canonical Forms]
The following are true:
\begin{itemize}
	\setlength\itemsep{-0.7em}
	\item If $\Gamma \vdash v: \Int$, then $v = l$ is a $\Nat$ constant.
	\item If $\Gamma \vdash v: \Bool$, then $b = l$ is a $\Bool$ constant.
\end{itemize}
\end{lemma}


\begin{lemma}[Substitution]
If $\Gamma, x: \tau' \vdash e: \tau$ and $\Gamma \vdash e': \tau'$ then $\Gamma \vdash [e'/x]e:  \tau$.
\end{lemma}

Proofs for these lemmas and theorems can be found in Appendix A.

To summarise, soundness is a property which says, generally, that if a type system says a program is well-typed, it will not encounter a runtime type-error. The corollary of this is also interesting to consider: if a program has no runtime type-error, will the type system accept it? This property is called \textit{completeness}, and almost no (interesting) type-systems are complete. This means a type system may reject type-safe programs. However, soundness guarantees that a type system will \textit{always} reject programs which are \textit{not} type-safe. Consider Figure 2.7., which demonstrates a type-safe Java program rejected by Java's type-system: the body of $\kwa{double}$ is type-safe, because the conditional will always execute $\kwa{return x+x}$. However, Java will reject this program.

\begin{figure}[h]
\vspace{-5pt}

\begin{lstlisting}
public int double(int x) {
   if (true) return x + x;
   else return true;
}
\end{lstlisting}
 
\vspace{-12pt}
\caption{A type-safe Java method which does not typecheck.}
\label{This is the label.}
\end{figure}

Throughout this report we will only be concerned with sound type systems, but it is important to recognise that these type systems are all \textit{conservative} because they may reject type-safe programs. One view of type-systems is that they ``calculate a kind of static  approximation to the run-time behaviours of the terms in a program'' \cite[p. 2]{tapl}. In order to approximate, simplifying assumptions must be made, and these simplifying assumptions are what make the type-system sound; but assumptions which are too generalising can make the system too conservative and of less practical use. This is an important trade-off we discuss in motivating $\epscalc$.



\section{ $\stlc$: Simply-Typed $\lambda$-Calculus}

The simply-typed $\lambda$-calculus $\stlc$ is a model of computation, first described by Alonzo Church \cite{church40}, based on the definition and application of functions. In this section we present a variation of $\stlc$ with subtyping and summarise its basic properties. Various $\lambda$-calculi serve as the basis for numerous functional programming languages, including $\epscalc$. This section gives us an opportunity to familiarise ourself with $\stlc$ to help introduce $\epscalc$.

\begin{figure}[h]
\vspace{-5pt}

\[
\begin{array}{lll}

\begin{array}{lllr}

e & ::= & ~ & exprs: \\
	& | & x & variable \\
	& | & e~e & application \\
	& | & v & value \\
	&&\\
	
v & ::= & ~ & values: \\
	& | & \lambda x: \tau . e & abstraction \\
	&&\\
	
\end{array}

& ~~~~~~ &

\begin{array}{lllr}

\tau & ::= & ~ & types: \\
	& | & B & base~type \\
	& | & \tau \rightarrow \tau & arrow~type \\
	&&\\
	
\Gamma & ::= & ~ & contexts: \\
	& | & \varnothing & empty~ctx. \\
	& | & \Gamma, x: \tau & var.~binding \\
	&&\\
	
\end{array}

\end{array}
\]

\vspace{-12pt}
\caption{Grammar for $\stlc$.}
\label{This is the label.}
\end{figure}

Types in $\stlc$ are either drawn from a set of base types B, or constructed using $\rightarrow$ (``arrow''). Given types $\tau_1$ and $\tau_2$, $\rightarrow$ can be used to compose a new type, $\tau_1 \rightarrow \tau_2$, which is the type of function taking $\tau_1$-typed terms as input to produce $\tau_2$-typed terms as output. For example, given $B = \{ \Bool, \Int \}$, the following are examples of valid types: $\Bool$, $\Int$, $\Bool \rightarrow \Bool$, $\Bool \rightarrow \Int$, $\Bool \rightarrow (\Bool \rightarrow \Int)$. Arrow is right-associative, so $\Bool \rightarrow \Bool \rightarrow \Int = \Bool \rightarrow (\Bool \rightarrow \Int)$. ``Arrow-type'' and ``function-type'' will be used interchangeably.

In addition to variables, there are function definitions (``abstraction'') and the application of a function to an expression (``application''). For example, $\lambda x: \Int . x$ is the identity function on integers. $(\lambda x: \Int . x) 3$ is the application of the identity function to the integer literal $3$. $(\lambda x: \Int . x) \true$ is the applciation of the identity function to a boolean literal, which is syntactically valid, but as we'll see is not well-typed. A more drastic example is $\true~3$, which is trying to apply $\true$ to $3$. Again, this is a syntactically valid term, but not well-typed because $\true$ is not a function.\\





\begin{figure}[h]

\fbox{$\Gamma \vdash e: \tau$}

\[
\begin{array}{c}


\infer[\textsc{(T-Var)}]
	{\Gamma, x: \tau \vdash x: \tau}
	{}
	
~~~
	
\infer[\textsc{(T-Abs)}]
	{\Gamma \vdash \lambda x: \tau_1.e : \tau_1 \rightarrow \tau_2}
	{\Gamma, x: \tau_1 \vdash e: \tau_2} \\[2ex]
	
	
\infer[\textsc{(T-App)}]
	{\Gamma \vdash e_1~e_2: \tau_3}
	{\Gamma \vdash e_1: \tau_2 \rightarrow \tau_3 & \Gamma \vdash e_2: \tau_2}
	~~~
\infer[\textsc{T-Subsume}]
	{\Gamma \vdash e: \tau_2}
	{\Gamma \vdash e: \tau_1 & \tau_1 <: \tau_2 }

\end{array}
\]

	
\fbox{$\tau <: \tau$}

	
\[
\begin{array}{c}


\infer[\textsc{(S-Arrow)}]
	{\tau_1 \rightarrow \tau_2 <: \tau_1' \rightarrow \tau_2'}
	{\tau_1' <: \tau_1 & \tau_2 <: \tau_2'}\\[2ex]


\end{array}
\]

\vspace{-12pt}
\caption{Static rules for $\stlc$.}
\label{This is the label.}
\end{figure}

Static rules for $\stlc$ are summarised in Figure 2.8. \textsc{T-Var} states that a variable bound in some context can be typed as its binding. \textsc{T-Abs} states that a function can be typed in $\Gamma$ if $\Gamma$ can type the body of the function when the function's argument has been bound. \textsc{T-App} states that an application is well-typed if the left-hand expression is a function (has an arrow-type $\tau_2 \rightarrow \tau_3$) and the right-hand expression has the same type as the function's input ($\tau_2$).

\textsc{T-Subsume} is the rule which says you may a type a term more generally as any of its supertypes. For example, if we had base types $\Int$ and $\Real$, and a rule specifying $\Int <: \Real$, a term of type $\Int$ can also be typed as $\Real$. This allows programs such as $(\lambda x: \Real. x)~3$ to type, as shown in Figure 2.9.

\begin{figure}[h]


    \begin{prooftree*}
        \Infer0[\textsc{(T-Var)}]{x: \Real \vdash x: \Real}
        \Infer1[\textsc{(T-Abs)}]{\vdash \lambda x: \Real . x : \Real \rightarrow \Real }
        
        \Hypo{\vdash 3: \Int}
        \Hypo{\Int <: \Real}
        \Infer2[\textsc{(T-Subsume)}]{\vdash 3: \Real}
        
        \Infer2[\textsc{(T-App)}]{\vdash (\lambda x: \Real . x)~3 : \Real}
 	\end{prooftree*}
 	
\vspace{-12pt}
\caption{Derivation tree showing how \textsc{T-Subsume} can be used.}
\label{This is the label.}
\end{figure}
 


The only subtyping rule we provide is \textsc{S-Arrow}, which describes when one function is a subtype of another. Note how the subtyping relation on the input types is reversed from the subtyping relation on the functions. This is called \textit{contravariance}. Contrast this with the relation on the output type, which preserves the order. That is called \textit{covariance}. Arrow-types are contravariant in their input and covariant in their output.

This presentation has no subtyping rules without premises (axioms), which means there is no way to actually prove a particular subtyping judgement. In practice, we add subtyping axioms for the base-types we have chosen as primitive in our calculus. For example, given base types $\Int$ and $\kwa{Real}$, we might add $\Real <: \Int$ as a rule. This is largely an implementation detail particular to your chosen set of base-types, so we give no subtyping axioms here (but will later when describing $\epscalc$).

\begin{figure}[h]

\bm{$\kwa{substitution :: e \times e \times v \rightarrow e}$}

\begin{itemize}
	\setlength\itemsep{-0.7em}
	\item[] $[ v/y]x =  v$, if $x = y$
	\item[] $[ v/y]x = x$, if $x \neq y$
	\item[] $[ v/y](\lambda x:  \tau.  e) = \lambda x:  \tau.[ v/y] e$, if $y \neq x$ and $y$ does not occur free in $ e$
	\item[] $[ v/y]( e_1~ e_2) = ([ v/y] e_1)([ v/y] e_2)$
\end{itemize}

\vspace{-12pt}
\caption{Substitution for $\stlc$.}
\label{This is the label.}
\end{figure}

Substitution in $\stlc$ follows the same conventions as it does in $\calc$. Substitution on an application is the same as substitution on its sub-expressions. Substitution on a function involves substitution on the function body.

\begin{figure}[h]

\noindent
\fbox{$e \longrightarrow e$}

\[
\begin{array}{c}

\infer[\textsc{(E-App1)}]
	{ e_1  e_2 \longrightarrow  e_1'  e_2~|~\varepsilon}
	{ e_1 \longrightarrow  e_1'~|~\varepsilon}
	~~~
\infer[\textsc{(E-App2)}]
	{ v_1  e_2 \longrightarrow  v_1  e_2'~|~\varepsilon} 
	{ e_2 \longrightarrow  e_2'~|~\varepsilon}\\[2ex]
	
\infer[\textsc{(E-App3)}]
	{ (\lambda x:  \tau. e)  v_2 \longrightarrow [ v_2/x] e~|~\varnothing }
	{}\\[2ex]
	
\end{array}
\]

\vspace{-12pt}
\caption{Dynamic rules for $\stlc$.}
\label{This is the label.}
\end{figure}

Applications are the only reducible expressions in $\stlc$. Such an expression is reduced by first reducing the left subexpression (\textsc{E-App1}). For a well-typed expression, this will always be a function. Once that is a value, the right subexpression is reduced (\textsc{E-App2}). When both subexpressions are values, the right subexpression replaces the formal argument of the function via substitution. The multi-step rules for $\stlc$ are identical to those in $\calc$.

The soundness property for $\stlc$ is as follows.

\begin{theorem}[$\stlc$ Soundness]
If $\Gamma \vdash e_A: \tau_A$ and $e_A \longrightarrow^* e_B$, then $\Gamma \vdash e_B: \tau_B$, where $\tau_B <: \tau_A$.
\end{theorem}

Note how with the inclusion of subtyping rules, the type after reduction can get more specific than the type before reduction, but never less specific. This is in contrast to $\calc$, where the type remains the same.

$\stlc$ is also strongly-normalizing, meaning that well-typed terms always halt i.e. they eventually yield a value. As a consequence it is \textit{not} Turing complete, meaning there are certain computer programs which cannot be written in $\stlc$. By comparison, the \textit{untyped} $\lambda$-calculus is known to be Turing complete \cite{kleene43}. The essential ingredient missing from $\stlc$ is a means of general recursion. In mainstream languages such as Java, this is realised by constructs like the $\kwa{while}$ loop; in the untyped $\lambda$-calculus by the Y-combinator. $\stlc$ can be made Turing-complete by adding a $\kwa{fix}$ operator which mimics the Y-combinator.

Turing-completeness is an essential property for practical, general-purpose programming languages. However, the key contribution of this report is in the static rules of $\epscalc$, and not the expressive power of its dynamic semantics. Therefore we acknowledge this practical short-coming, but leave the basis of $\epscalc$ as a Turing-incomplete language to reduce the number of rules and simplify its presentation.

\textbf{Revisit this depending on how you encode types and stuff in $\epscalc$}


\section{Effect Systems}

In the previous section we looked at how the static rules of a language might make a judgement such as $\Gamma \vdash e: \tau$, which ascribes the type $\tau$ to program $e$. This expresses a certain about what the runtime behaviour of the program is: namely that successive reductions of $e$ will produce terms of type $\tau$, and that this sequence of reductions will never get stuck due to a runtime type-error.

One extension to classical type systems is to incorporate a theory of \textit{effects}. A \textit{type-and-effect} system can ascribe a type and an effect to a piece of code, the effect component of which specifies intensional information about what will happen during the execution of the program \cite{nielson99}. For example, a judgement like $\Gamma \vdash e: \tau~\kw{with} \{ \kwa{File.write} \}$ means that successive reductions of $e$ will result in terms of type $\tau$, and during execution might write to a file. This tells us extra information about what can happen during runtime.

\textbf{Talk about use of effect systems. Mention the basic effect system we will introduce.}

\subsection{ETL: Effect-Typed Language}




\section{The Capability Model}

A \textit{capability} is a unique, unforgeable reference, giving its bearer permission to perform some operation \cite{dennis66}. A piece of code $S$ has \textit{authority} over a capability $C$ if it can directly invoke the operations endowed by $C$; it has \textit{transitive authority} if it can indirectly invoke the operations endowed by a capability $C$ (for example, by deferring to another piece of code with authority over $C$).

In a capability model, authority can only proliferate in the following ways \cite{miller06}:

\begin{enumerate}
	\item By the initial set of capabilities passed into the program (initial conditions).
	\item If a function or object is instantiated by its parent, the parent gains a capability for its child (parenthood).
	\item If a function or object is instantiated by a parent, the parent may endow its child with any capabilities it possesses (endowment).
	\item A capability may be transferred via method-calls or function applications (introduction).
\end{enumerate}

The rules of authority proliferation are summarised as: ``only connectivity begets connectivity''.

Primitive capabilities are called \textit{resources}. Resources model those initial capabilities passed into the runtime from the system environment. A capability is either a resource, or a function or object with (potentially transitive) authority over a capability. An example of a resource might be a particular file. A function which manipulates that file (for example, a logger) would also be a capability, but not a resource. Any piece of code which uses a capability, directly or indirectly, is called \textit{impure}. For example, $\kwa{\lambda x: \kwa{Int} .~x}$ is pure, while $\kwa{\lambda f: File .~f.log(``error~message'')}$ is impure.

A relevant concept in the design of capability-based programming languages is \textit{ambient authority}. This is a kind of exercise of authority over a capability $C$ which has not been explicitly \cite{miller03}. Figure 2.4. gives an example in Java, where a malicious implementation of $\kwa{List.add}$ attempts to overwrite the user's $\kwa{.bashrc}$ file. $\kwa{MyList}$ gains this capability by importing the $\kwa{java.io.File}$ class, but its use of files is not immediate from the signature of its functions.

Ambient authority is a challenge to POLA because it makes it impossible to determine from a module's signature what authority is being exercised. From the perspective of $\kwa{Main}$, knowing that $\kwa{MyList.add}$ has a capability for the user's $\kwa{.bashrc}$ file requires one to inspect the source code of $\kwa{.bashrc}$; a necessity at odds with the circumstances which often surround untrusted code and code ownership.

\begin{figure}[h]

\begin{lstlisting}
import java.io.File;
import java.io.IOException;
import java.util.ArrayList;

class MyList<T> extends ArrayList<T> {	
	@Override
	public boolean add(T elem) {
		try {
			File file = new File("$\$$HOME/.bashrc");
			file.createNewFile();
		} catch (IOException e) {}
		return super.add(elem);
	}	
}
\end{lstlisting}

\begin{lstlisting}
import java.util.List;

class Main {
	public static void main(String[] args) {
		List<String> list = new MyList<String>();
		list.add(``doIt'');
	}
}
\end{lstlisting}

\vspace{-12pt}
\caption{$\kwa{Main}$ exercises ambient authority over a $\kwa{File}$ capability.}
\label{A sample. }
\end{figure}

A language is \textit{capability-safe} if it satisfies this capability model and disallows ambient authority. Some examples include E, Js, and Wyvern. \textbf{Get citations.}

\section{First-Class Modules}

The exact way in which modules work is language-dependent, but we are particularly interested in languages with a first-class module systems. First-class modules are important in capability-safe languages because they mean capability-safe reasoning operates across module boundaries. Because modules are first-class, they must be instantiated like regular objects. They must therefore select their capabilities, and be supplied those capabilities by the proliferation rules of the capability model. In practice, first-class modules can be achieved by having module declarations desugar into an underlying lambda or object representation. This generally requires an ``intermediate representation'' of the language, which is simpler than the one in which programmers write.

Java is an example of a mainstream language whose modules are not first-class. Scala has first-class modules \cite{odersky16}, but is not capability-safe. Smalltalk is a dynamically-typed capability-safe language with first-class modules \cite{bracha10}. Wyvern is a statically-typed capability-safe language with first-class modules \cite{kurilova16}.
