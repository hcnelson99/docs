\chapter{Introduction}\label{C:intro}

Good software is distinguished from bad software by design qualities such as security, maintainability, and performance. We are interested in how the design of a programming language and its type system can make it easier to write secure software.

There are different situations where we may not trust code. One example is in any development environment adhering to ideas of \textit{code ownership}, wherein groups of developers function as experts over certain components in the system \textbf{REFERENCE}. When one group's components must interact with another's, they can make false assumptions or violate their internal constraints by using them incorrectly. This can break correctness or leave components in a malconfigured state, putting the whole system at risk.  Another setting involves applications which allow third-party plug-ins, in which case third-party code could be written by anyone, including the untrustworthy. One kind of application which does this is the web mash-up, which brings together several existing, disparate services into one system. In both cases we want the entire system to function securely, despite the existence of untrustworthy components.

It is difficult to determine if a piece of code is trustworthy, but a range of techniques might be used. One approach is to \textit{sandbox} the untrusted code inside a virtual environment. If anything goes wrong, damage is theoretically limited to the virtual environment, but in practice, this approach has many vulnerabilities \cite{coker15, maass16, watson07, schreuders13}. On the other hand, verification techniques allow for a robust analysis of the behaviour of code, but are heavyweight and require the developers using them to have a deep understanding of the techniques being employed \cite{kneuper97}. Furthermore, verification requires one to supply a complete specification of the system, which may itself be an undefined or evolving artifact during the development process. Lightweight analyses, such as type systems, are easy for the developer to use, but existing languages lack adequate controls for detecting and isolating untrustworthy components \cite{chen07, ter-louw08}. A qualitative approach might instead be employed, where software is developed according to best-practice guidelines. One such guideline is the \textit{principle of least authority}: that software components should only have access to the information and resources necessary for their purpose \cite{saltzer74}. For example, a logger module, which needs only to append to a file, should not have arbitrary read-write access. Another is \textit{privilege separation}, where the division of a program into components is informed by what resources are needed and how they are to be propagated \cite{saltzer75}. This report is interested in the class of lightweight analyses, and in particular how type systems could be used to reject unsafe programs or put developers in a more informed position to make qualitative assessments about their code.

One approach to privilege separation is the capability model. A \textit{capability} is an unforgeable token granting its bearer permission to perform some operation \cite{dennis66}. For example, a system resource like a file or socket can only be used through a capability granting operations on it. Capabilities also encapsulate the source of \textit{effects}, which describe intensional details about the way in which a program executes \cite{nielson99}. For example, a logger might $\kwa{append}$ to a $\kwa{File}$, and so executing its code would incur the $\kwa{File.append}$ effect. In the capability model, this would require the logger to possess a capability granting it the ability to append to files.

Although the idea of a capability is an old one in the access literature, there has been recent interest in the application of the idea to programming language design. Miller has identified the ways in which capabilities should proliferate to encourage \textit{robust composition} --- a set of ideas summarised as ``only connectivity begets connectivity'' \cite{miller06}. In this paradigm, actors in a program are explicitly parametrised by what capabilities they use. This enables one to reason about what privileges a component might exercise by examining its interface. Building on these ideas, Maffeis et. al. formalised the notion of a \textit{capability-safe} language, showing a subset of Caja (a JavaScript implementation) is capability-safe \cite{maffeis10}.

Effect systems were introduced by Lucassen and Gifford for the purposes of optimising pure code \cite{lucassen88}. They have also been applied to problems such as determining which functions might be invoked in a program \cite{tang94}, or determining which regions in memory may be accessed or updated \cite{talpin94}. Knowing what effects a piece of code might incur allows a developer to determine if code is trustworthy before executing it. This can be qualitatively assessed by comparing the static approximation of its effects to its expected least authority --- a ``logger'' implementation which writes to a $\kwa{Socket}$ is not to be trusted!

Despite these benefits, effect systems have seen little use in mainstream programming languages. Rytz et. al. believe verbosity is the main reason \cite{rytz2012}. Successive works have focussed on reducing the developer overhead through techniques such as effect-inference, but the benefit of capabilities for enabling effect-inference has not received much attention. Because capabilities encapsulate the source of effects, and because capability-safety impose constraints on how they propagate through a system, the task of determining what effects might be incurred by a piece of code is simplified. This is the key contribution of this report: the idea that capability-safety facilitates a low-cost effect system with minimal user overhead. 

We begin this report by discussing preliminary concepts involving the formal definition of programming languages, effect systems, and Miller's capability model. Chapter 3 introduces the Operation Calculus $\opercalc$, a typed, effect-annotated lambda calculus with a simple notion of capabilities and effect. Dropping the requirement that all code in a program must be effect-annotated, we develop the Capability Calculus $\epscalc$, which permits the nesting of unannotated code inside annotated code in a controlled, capability-safe manner with a new $\kwa{import}$ construct. A safe inference about the unannotated code can be made at these junctions. In chapter 4 we demonstrate how $\epscalc$ can model practical examples, finishing with a summary and comparison of some of the existing work in this area.